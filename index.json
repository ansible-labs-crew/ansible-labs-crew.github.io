[
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/",
	"title": "Ansible Getting Started",
	"tags": [],
	"description": "",
	"content": "Ansible Getting Started   Exercise 1 - Check the Prerequisites\n  Exercise 2 - Running ad hoc Commands\n  Exercise 3 - Writing Your First Playbook\n  Exercise 4 - Using Variables\n  Exercise 5 - Conditionals, Handlers and Loops\n  Exercise 6 - Templates\n  Exercise 7 - Bonus Labs\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/1-setup/",
	"title": "Check the Prerequisites",
	"tags": [],
	"description": "",
	"content": "Your Lab Environment In this lab you work in a pre-configured lab environment. You will have access to the following hosts:\n   Role Inventory name     Ansible Control Host ansible   Managed Host 1 node1   Managed Host 2 node2   Managed Host 3 node3    The lab environments in this session have a \u0026lt;LABID\u0026gt; and are separated by numbered student\u0026lt;N\u0026gt; accounts. Follow the instructions given by the lab facilitators to receive the values for student\u0026lt;N\u0026gt; and \u0026lt;LABID\u0026gt;!\n On the lab landing page you\u0026rsquo;ll find the URLs you need to access complete with student number and lab ID already filled in.\n Accessing your Lab Environment Your main points of contact with the lab is VS Code, providing a VSCode-experience in your browser.\nNow open VS Code server using the VS Code access link from the lab landing page or use this link in your browser by replacing \u0026lt;N\u0026gt; by your student number and the \u0026lt;LABID\u0026gt;:\nhttps://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com  Use the password provided on the lab landing page to login into the VS Code server web UI, you can close the Welcome tab. Now open a new terminal by heading to the menu item Terminal at the top of the page and select New Terminal. A new section will appear in the lower half of the screen and you will be greeted with a prompt:\nIf unsure how to use VS Code server, read the Visual Studio Code Server introduction, to learn more about how to create and edit files, and to work with the Terminal.\nCongrats, you now have a shell terminal on your Ansible control node. From here you run commands or access the other hosts in your lab environment if the lab task requires it.\nNow in the terminal become root:\n[ec2-user@autoctl1 ~]$ sudo -i  Most prerequisite tasks have already been done for you:\n  Ansible software is installed\n  sudo has been configured on the managed hosts to run commands that require root privileges.\n  Check Ansible has been installed correctly (your actual Ansible version might differ):\n[root@bastion.\u0026lt;GUID\u0026gt;.internal ~]# ansible --version ansible 2.9.6 [...]  Ansible is keeping configuration management simple. Ansible requires no database or running daemons and can run easily on a laptop. On the managed hosts it needs no running agent.\n Log out of the root account again:\n[root@bastion.\u0026lt;GUID\u0026gt;.internal ~]# exit logout  In all subsequent exercises you should work as the student\u0026lt;GUID\u0026gt; user on the control node if not explicitly told differently.\n Working the Labs You might have guessed by now this lab is pretty command line-centric…​ :-)\nDon’t type everything manually, use copy \u0026amp; paste from the browser when appropriate. But do still take time to think and understand.\nIn the lab guide commands you are supposed to run are shown with or without the expected output, whatever makes more sense in the context.\n Challenge Labs You will soon discover that many chapters in this lab guide come with a \u0026ldquo;Challenge Lab\u0026rdquo; section. These labs are meant to give you a small task to solve using what you have learned so far. The solution of a challenge task is shown beneath the task in a fold-out.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "This chapter will introduce you to your lab environment and introduce you to the importance of Ansible Collections, what they are and where existing collections can be found and consumed.\nYour Lab Environment In this lab you work in a pre-configured lab environment. You will have access to the following hosts:\n   Role URL for External Access (if applicable) Hostname Internal     automation controller autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com autoctl1.\u0026lt;GUID\u0026gt;.internal   Visual Code Web UI bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com    Managed RHEL8 Host 1  node1.\u0026lt;GUID\u0026gt;.internal   Managed RHEL8 Host 2  node2.\u0026lt;GUID\u0026gt;.internal   Managed RHEL8 Host 3  node3.\u0026lt;GUID\u0026gt;.internal    The lab environments in this session have a \u0026lt;LABID\u0026gt; and are separated by numbered student\u0026lt;N\u0026gt; accounts. Follow the instructions given by the lab facilitators to receive the values for student\u0026lt;N\u0026gt; and \u0026lt;LABID\u0026gt;!\n On the lab landing page you\u0026rsquo;ll find the URLs you need to access complete with student number and lab ID already filled in.\n Accessing your Lab Environment Your main points of contact with the lab is VS Code, providing a VSCode-experience in your browser.\nNow open VS Code server using the VS Code access link from the lab landing page or use this link in your browser by replacing \u0026lt;N\u0026gt; by your student number and the \u0026lt;LABID\u0026gt;:\nhttps://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com Use the password provided on the lab landing page to login into the VS Code server web UI, you can close the Welcome tab. Now open a new terminal by heading to the menu item Terminal at the top of the page and select New Terminal. A new section will appear in the lower half of the screen and you will be greeted with a prompt:\nIf unsure how to use VS Code server, read the Visual Studio Code Server introduction, to learn more about how to create and edit files, and to work with the Terminal.\nCongrats, you now have a shell terminal on your Ansible control node. From here you run commands or access the other hosts in your lab environment if the lab task requires it.\nNow in the terminal become root:\n[ec2-user@autoctl1 ~]$ sudo -i  Most prerequisite tasks have already been done for you:\n  Ansible software is installed\n  sudo has been configured on the managed hosts to run commands that require root privileges.\n  Check Ansible has been installed correctly (your actual Ansible version might differ):\n[bastion.\u0026lt;GUID\u0026gt;.internal ~]# ansible --version ansible 2.9.13 [...]  Log out of the root account again:\n[bastion.\u0026lt;GUID\u0026gt;.internal ~]# exit logout  In all subsequent exercises you should work as the student\u0026lt;GUID\u0026gt; user on the control node if not explicitly told differently.\n What are Collections and why should I care? Ansible Collections are a new distribution format for Ansible content that can include playbooks, roles, modules, and plugins. Modules are moved from the core Ansible repository into collections living in repositories outside of the core repository. This change in the content delivery process will allow Ansible to keep up the tremendous success and, coming with it, growth in content.\n Before collections, module creators had to wait for their modules to be included in an upcoming Ansible release or had to add them to roles, which made consumption and management more difficult. By distributing modules packaged in Ansible Content Collections along with roles, documentation and even playbooks, content creators are now able to move as fast or conservative as the technology they manage demands.  Example: A public cloud provider could make new functionality of an existing service available, that could be rolled out along with the ability to automate the new functionality with Ansible. With Ansible Collections the author doesn\u0026rsquo;t have to wait for the next Ansible release and can instead roll out the new content independently. Prior to Ansible Collections the author had to wait for the next Ansible release.\nFor Ansible users, the benefit is that updated content can continuously be made available. Managing content this way also becomes easier as modules, plugins, roles, and docs that belong together are packaged together and versioned.\nFully Qualified Collection Name Ansible Collection names are a combination of two components. The first part is the name of the author who wrote and maintains the Ansible Collection. The second part is the name of the Ansible Collection. This allows one author to have multiple Collections. It also allows multiple authors to have Ansible Collections with the same name.\n\u0026lt;author\u0026gt;.\u0026lt;collection\u0026gt;  These are examples for Ansible Collection names:\n  ansible.posix\n  geerlingguy.k8s\n  theforeman.foreman\n  To identify a specific module in an Ansible Collection, we add the name of it as the third part:\n\u0026lt;author\u0026gt;.\u0026lt;collection\u0026gt;.\u0026lt;module\u0026gt;  Valid examples for a fully qualified Ansible Collection Name:\n  ansible.posix.selinux\n  geerlingguy.k8s.kubernetes\n  theforeman.foreman.user\n  Understand Collections Lookup Ansible Collections use a simple method to define collection namespaces. If your playbook loads collections using the collections key and one or more roles, then the roles will not inherit the collections set by the playbook.\nThis leads to the main topic of this exercise: roles have an independent collection loading method based on the role\u0026rsquo;s metadata. To control collections search for the tasks inside the role, users can choose between two approaches:\n  Approach 1: Pass a list of collections in the collections field inside the meta/main.yml file within the role. This will ensure that the collections list searched by the role will have higher priority than the collections list in the playbook. Ansible will use the collections list defined inside the role even if the playbook that calls the role defines different collections in a separate collections keyword entry.\n# myrole/meta/main.yml collections: - my_namespace.first_collection - my_namespace.second_collection - other_namespace.other_collection   Approach 2: Use the collection fully qualified collection name (FQCN) directly from a task in the role. In this way the collection will always be called with its unique FQCN, and override any other lookup in the playbook\n- name: Create an EC2 instance using collection by FQCN amazon.aws.ec2: key_name: mykey instance_type: t2.micro image: ami-123456 wait: yes group: webserver count: 3 vpc_subnet_id: subnet-29e63245 assign_public_ip: yes   Roles defined within a collection always implicitly search their own collection first, so there is no need to use the collections keyword in the role metadata to access modules, plugins, or other roles.\nIn the following chapters of this lab you will learn how collections work and see different examples to see how it works.\nSince the Ansible Collection lookup could can deliver unexpected results, it is best practice to always use the fully qualified collection name.\n "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/1-intro/",
	"title": "Introduction to automation controller",
	"tags": [],
	"description": "",
	"content": "Automation controller? Where is Ansible Tower? During the planning of Red Hat Ansible Automation Platform 2 the decision was made to rename a number of components. The main reason behind this is to make it clear that e.g. Ansible Engine and Ansible Tower are parts of an comprehensive automation platform.\nSo the artist formerly known as Ansible Tower is now called \u0026ldquo;automation controller\u0026rdquo; (without capitals!).\nAutomation controller basically is an API for Ansible Automation, most users will get in touch with it through the web-based UI which uses the API underneath. It provides the following features:\n  A user-friendly dashboard\n  Role based access control\n  One-click automation templates\n  Management of dynamic inventory sources\n  Automation workflows with approval\n  A solid audit track (\u0026ldquo;who did what when\u0026rdquo;)\n  And much more\u0026hellip; as you\u0026rsquo;ll learn in this lab!\nYour automation controller Lab Environment In this lab you work in a pre-configured lab environment. You will have access to the following hosts:\n   Role URL for External Access (if applicable) Hostname Internal     Automation controller autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com autoctl1.\u0026lt;GUID\u0026gt;.internal   Visual Code Web UI bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com    Managed RHEL8 Host 1  node1.\u0026lt;GUID\u0026gt;.internal   Managed RHEL8 Host 2  node2.\u0026lt;GUID\u0026gt;.internal   Managed RHEL8 Host 3  node3.\u0026lt;GUID\u0026gt;.internal    The lab environments in this session have a \u0026lt;SANDBOXID\u0026gt; and are separated by numbered \u0026lt;GUID\u0026gt; accounts. You will be able to access the hosts using the external hostnames. Internally the hosts have different names as shown above. Follow the instructions given by the lab facilitators to receive the values for \u0026lt;GUID\u0026gt; and \u0026lt;SANDBOXID\u0026gt;!\n Automation controller has already been installed and licensed for you, the web UI will be reachable over HTTP/HTTPS.\n In general, whenever you need a password, even without the placeholder explicitly written, it\u0026rsquo;s the same one.\n Working the Lab Some hints to get you started:\n  Don’t type everything manually, use copy \u0026amp; paste from the browser when appropriate. But don’t stop to think and understand… ;-)\n  To edit files or open a terminal window, we provide VS Code delivered by VS Code server, basically the great Visual Studio Code Editor running in your browser. It\u0026rsquo;s running on the bastion node and can be accessed through the URL https://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\n  Commands you are supposed to run are shown with or without the expected output, whatever makes more sense in the context.\n The command line can wrap on the HTML page from time to time. Therefore the output is often separated from the command line for better readability by an empty line. Anyway, the line you should actually run should be recognizable by the prompt. :-)\n Accessing your Lab Environment You\u0026rsquo;ll get the access information for your lab (URL\u0026rsquo;s, password) from your lab facilitator. Your main points of contact with the lab are the automation controller\u0026rsquo;s web UI and VS Code in your browser. You\u0026rsquo;ll use VS Code to:\n  Open virtual terminals\n  Edit files\n  Now open VS Code in your browser using the link provided or use this link by replacing \u0026lt;GUID\u0026gt; by your GUID (a four digits hexacode) and the \u0026lt;SANDBOXID\u0026gt;:\nhttps://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\nUse the password provided to login into the VS Code server web UI, you can close the Welcome tab. Now open a new terminal by heading to the menu item Terminal at the top of the page and select New Terminal. A new section will appear in the lower half of the screen and you will be greeted with a prompt:\nIf unsure about the usage, read the Visual Studio Code Server introduction, to learn more about how to create and edit files, and to work with the Terminal.\nThere is a known bug when using VSCode in the Chrome browser: Under some circumstances/locale settings the keyboard layout in the terminal window (not the visual editor) is mixed up. It works fine in Firefox, though.\n Direct Access using SSH Last but not least you can of course use SSH directly to access the bastion node when you have an SSH client ready to go and know your way around:\nssh lab-user@bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\nThe password is still the same.\nCongrats, you now have a shell terminal on your bastion node. From here you run commands or access the other hosts in your lab environment if a lab task requires it.\nThe user you are accessing the terminal as is lab-user, but your bastion node is setup to let you become root using sudo without a password.\n Managed Nodes hostnames As mentioned you can construct your internal hostnames with your \u0026lt;GUID\u0026gt;. But there is an easier way: On your bastion host you can find an Ansible inventory file for your environment. Just look at it in your VSCode terminal and you\u0026rsquo;ll get the internal hostnames:\n[lab-user@bastion ~]$ cat /etc/ansible/hosts Dashboard Let\u0026rsquo;s have a first look at the automation controller: Point your browser to the URL you were given, similar to https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com (replace \u0026lt;GUID\u0026gt; with your GUID (a four digits hexacode) and \u0026lt;SANDBOXID\u0026gt; with the sandbox ID) and log in as admin with the passwrod provided.\nThe web UI of the automation controller greets you with a dashboard giving an overview of your automation including:\n  Recent job activity\n  The number of managed hosts\n  Quick pointers to lists of hosts with problems.\n  The dashboard also displays real time data about the execution of tasks completed in playbooks.\nConcepts Before we dive further into using automation controller for your automation, you should get familiar with some concepts and naming conventions.\nProjects Projects are logical collections of Ansible playbooks in automation controller. These playbooks usually reside in a source code version control system supported by automation controller.\nInventories An Inventory is a collection of hosts against which jobs may be launched, the same as an Ansible inventory file you might know from working with Ansible on the command line. Inventories are divided into groups and these groups contain the actual hosts. Groups may be populated manually, by entering host names into automation controller, from one of automation controller’s supported cloud providers or through dynamic inventory scripts.\nCredentials Credentials are utilized by automation controller for authentication when launching Jobs against machines, synchronizing with inventory sources, and importing project content from a version control system. Automation controller credentials are imported and stored encrypted in automation controller, and are not retrievable in plain text on the command line by any user. You can grant users and teams the ability to use these credentials, without actually exposing the credential to the user.\nTemplates A job template is a definition and set of parameters for running an Ansible job. Job templates are useful to execute the same job many times. Job templates also encourage the reuse of Ansible playbook content and collaboration between teams. To execute a job, automation controller requires that you first create a job template.\nJobs A job is basically an instance of automation controller launching an Ansible playbook against an inventory of hosts.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/1-intro/",
	"title": "Introduction to your lab",
	"tags": [],
	"description": "",
	"content": "About this Lab You have already used Ansible Automation quite a bit and have started to look into the automation controller (formerly know as Ansible Tower, part of the Red Hat Ansible Automation Platform)? Or you are already using automation controller? Cool. We prepared this lab to give a hands-on introduction to some of the more advanced features of the automation controller. You’ll learn about:\n  Using command line tools to manage automation controller\n  automation controller clustering\n  Working with Instance Groups\n  Ways to provide inventories (importing inventory, dynamic inventory)\n  The Smart Inventory feature\n  Optional: How to structure Ansible content in Git repos\n  Optional: How to work with the automation controller API\n  So little time and so much to do… To be honest we got carried away slightly while trying to press all these cool features into a two-hours lab session. We decided to flag the last two chapters as \u0026ldquo;optional\u0026rdquo; instead of taking them out. If you find the time to run them, cool! If not, the lab guide will stay where it is, feel free to go through these lab tasks later (you don’t need an automation controller cluster for this).\n Want to Use this Lab after the event? Definitely, the lab documentation is available here:\nhttps://ansible-labs-crew.github.io/\nIf you want to contribute to the workshop to make it better, file an issue in our GitHub project.\nYour Ansible Automation Platform Lab Environment In this lab you work in a pre-configured lab environment. You will have access to the following hosts:\n   Role URL for External Access (if applicable) Hostname Internal     Automation controller node 1 autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com autoctl1.\u0026lt;GUID\u0026gt;.internal   Automation controller node 2 autoctl2.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com autoctl2.\u0026lt;GUID\u0026gt;.internal   Automation controller node 3 autoctl3.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com autoctl3.\u0026lt;GUID\u0026gt;.internal   Visual Code Web UI bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com    Database Node  pgdb.\u0026lt;GUID\u0026gt;.internal   Managed RHEL8 Host 1  node1.\u0026lt;GUID\u0026gt;.internal   Managed RHEL8 Host 2  node2.\u0026lt;GUID\u0026gt;.internal   Managed RHEL8 Host 3  node3.\u0026lt;GUID\u0026gt;.internal    The lab environments in this session have a \u0026lt;SANDBOXID\u0026gt; and are separated by numbered \u0026lt;GUID\u0026gt; accounts. You will be able to access the hosts using the external hostnames. Internally the hosts have different names as shown above. Follow the instructions given by the lab facilitators to receive the values for \u0026lt;GUID\u0026gt; and \u0026lt;SANDBOXID\u0026gt;!\n Ansible Automation Platform (AAP) has already been installed and licensed for you, the web UI will be reachable over HTTP/HTTPS.\n In general, whenever you need a password, it\u0026rsquo;s the same one provided for lab access.\n As you can see the lab environment is pretty extensive. You basically have:\n  A bastion host running the VSCode server.\n  A three-node automation controller cluster with a separate DB host, accessed via SSH or web UI\n  Three managed RHEL 8 hosts\n  Working the Lab Some hints to get you started:\n  Don’t type everything manually, use copy \u0026amp; paste from the browser when appropriate. But don’t stop to think and understand… ;-)\n  To edit files or open a terminal window, we provide VS Code, basically the great VSCode Editor running in your browser. It\u0026rsquo;s running on the bastion node and can be accessed through the URL https://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\n  Commands you are supposed to run are shown with or without the expected output, whatever makes more sense in the context.\n The command line can wrap on the HTML page from time to time. Therefore the output is often separated from the command line for better readability by an empty line. Anyway, the line you should actually run should be recognizable by the prompt. :-)\n Accessing your Lab Environment You\u0026rsquo;ll get the access information for your lab (URL\u0026rsquo;s, password) from your lab facilitator. Your main points of contact with the lab are the automation controller node\u0026rsquo;s web UI and VS Code in your browser. You\u0026rsquo;ll use VS Code to:\n  Open virtual terminals\n  Edit files\n  Now open VS Code in your browser using the link provided or use this link by replacing \u0026lt;GUID\u0026gt; by your GUID (a four digits hexacode) and the \u0026lt;SANDBOXID\u0026gt;:\nhttps://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\nUse the password provided to login into the VS Code server web UI, you can close the Welcome tab. Now open a new terminal by heading to the menu item Terminal at the top of the page and select New Terminal. A new section will appear in the lower half of the screen and you will be greeted with a prompt:\nIf unsure about the usage, read the Visual Studio Code Server introduction, to learn more about how to create and edit files, and to work with the Terminal.\nThere is a known bug when using VSCode in the Chrome browser: Under some circumstances/locale settings the keyboard layout in the terminal window (not the visual editor) is mixed up. It works fine in Firefox, though.\n Direct Access using SSH Last but not least you can of course use SSH directly to access the bastion node when you have an SSH client ready to go and know your way around:\nssh lab-user@bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\nThe password is still the same.\nCongrats, you now have a shell terminal on your bastion node. From here you run commands or access the other hosts in your lab environment if a lab task requires it.\nThe user you are accessing the terminal as is lab-user, but your bastion node is setup to let you become root using sudo without a password.\n Managed Nodes hostnames As mentioned you can construct your internal hostnames with your \u0026lt;GUID\u0026gt;. But there is an easier way: On your bastion host you can find an Ansible inventory file for your environment. Just look at it in your VSCode terminal and you\u0026rsquo;ll get the internal hostnames:\n[lab-user@bastion ~]$ cat /etc/ansible/hosts Install Ansible Before we can get started writing Ansible Playbooks, we have to install Ansible first. We did all the preparation for you, so the installation is super easy:\n[lab-user@bastion ~]$ sudo yum -y install ansible "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/",
	"title": "Ansible automation controller getting started",
	"tags": [],
	"description": "",
	"content": "Ansible automation controller getting started   Exercise 1 - Introduction to Controller\n  Exercise 2 - Inventories, credentials and ad hoc commands\n  Exercise 3 - Projects \u0026amp; job templates\n  Exercise 4 - Surveys\n  Exercise 5 - Role-based access control\n  Exercise 6 - Workflows\n  Exercise 7 - Wrap up\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/2-using-collections-from-playbooks/",
	"title": "Collections from Playbook",
	"tags": [],
	"description": "",
	"content": "For the following exercise, we will use a collection written by the Ansible Core Team. The name of the author is therefore \u0026ldquo;ansible\u0026rdquo;. You can find a list of all modules and collections written by the Ansible Core Team on Ansible Galaxy. Head over there and have a good look around!\nAs you can see they maintain several collections and roles. One of their collections is called \u0026ldquo;posix\u0026rdquo; and we can find the documentation and additional details on the Ansible Galaxy POSIX Collection page.\nOne of the modules provided by this collection allows us to manage SELinux settings. The fully qualified collection name for this module is therefore ansible.posix.selinux.\nYou can find more details about using collections in the Ansible Documentation.\nInstall the Ansible Collection The ansible.posix.selinux module which we want to use for this exercise, is part of the ansible.posix collection. We have to install this collection first, before we can use its modules. The ansible-galaxy command line tool can be used to automate the installation. It is preconfigured to search for roles and collections on Ansible Galaxy so we can just specify the collection name and it will take care of the rest.\nBring up a browser window with VS Code and open a terminal. In the terminal run:\n[ec2-user@autoctl1 ~]$ ansible-galaxy collection install ansible.posix  This will install the collection on your system, only if it wasn\u0026rsquo;t installed before. To force the installation, for example to make sure you\u0026rsquo;re on the latest version, you can add the force switch -f.\n[ec2-user@autoctl1 ~]$ ansible-galaxy collection install -f ansible.posix  This will always download and install the latest version, even if it was already up to date. Ansible Collections can have dependencies for other Ansible Collections as well - if you want to make sure those dependencies are refreshed as well, you can use the --force-with-deps switch.\nBy default the installation is stored in your local ~/.ansible directory. This can be overwritten by using the -p /path/to/collection switch. Keep in mind though that ansible-playbook will only use this directory, if you change your ansible.cfg accordingly. To check your current configuration, you can dump your configuration and search for collection.\n[ec2-user@autoctl1 ~]$ ansible-config dump | grep -i collection COLLECTIONS_PATHS(default) = [\u0026#39;/home/student\u0026lt;GUID\u0026gt;/.ansible/collections\u0026#39;, \u0026#39;/usr/share/ansible/collections\u0026#39;] Browse the Documentation The ansible-doc command only searches the system directories for documentation. You can still use it though to read up on modules you installed from Ansible Collections by using the fully qualified collection name.\nLet\u0026rsquo;s have a look at the module documentation for the selinux module of the ansible.posix collection, which we are going to use in the next part of the exercise:\n[ec2-user@autoctl1 ~]$ ansible-doc ansible.posix.selinux \u0026gt; SELINUX (/home/student\u0026lt;GUID\u0026gt;/.ansible/collections/ansible_collections/ansible/posix/plugins/modules/selinux.py)  Note the start of the output, you can see the location of the module. For educational purposes run the ansible-doc again, but this time not specifying the collection:\n[ec2-user@autoctl1 ~]$ ansible-doc selinux \u0026gt; SELINUX (/usr/lib/python3.6/site-packages/ansible/modules/system/selinux.py)  You can see how the command this time pulled the documentation for the module that was installed with Ansible.\nDepending on your screen resolution you might have to press q to leave the documentation viewer.\n Write an Ansible Playbook We want to use the SELinux module to make sure it is configured in enforcing mode. SELinux is a kernel feature which brings extra security to our Linux system and it is highly recommended to always keep it enabled and in enforcing mode. If you\u0026rsquo;re new to SELinux, there is a nice article on What is SELinux to get you started.\nLet\u0026rsquo;s write a simple playbook which enables SELinux and sets it to enforcing mode on the local machine. In this lab you can use the visual VS Code editor or run an editor of your choice from the terminal. Create the Playbook enforce-selinux.yml with the following content:\n--- - name: set SELinux to enforcing hosts: localhost become: yes tasks: - name: set SElinux to enforcing ansible.posix.selinux: policy: targeted state: enforcing Make sure you save the playbook as enforce-selinux.yml in your student users home directory.\nPay special attention to the module name. Typically you would see something like selinux, but since we are using a module provided by an Ansible Collection, we have to specify the fully qualified collection name.\n Test the playbook Noe let\u0026rsquo;s run the Playbook and see what happens:\n[ec2-user@autoctl1 ~]$ ansible-playbook enforce-selinux.yml  You should see output like this:\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [set SELinux to enforcing] *********************************************************************************** TASK [Gathering Facts] ******************************************************************************************** ok: [localhost] TASK [set SElinux to enforcing] *********************************************************************************** ok: [localhost] PLAY RECAP ******************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0  If SELinux was not set to enforcing mode before, you might see \u0026ldquo;changed\u0026rdquo; instead of \u0026ldquo;ok\u0026rdquo;. If it did say \u0026ldquo;changed\u0026rdquo; and you run it a second time, you should now see \u0026ldquo;ok\u0026rdquo; - the magic of Ansible idempotency.\nSimplify the namespace If you use many modules from Ansible Collections in your Playbook, the \u0026lt;author\u0026gt;.\u0026lt;collection\u0026gt; prefix can become quite annoying and reading your Playbook can become harder as well.\nYou can use the collections keyword to skip defining the namespace with every task. In your terminal edit the Playbook enforce-selinux.yml to look like this, basically adding the collections: section and changing the module name from FQCN to the simple module name:\n--- - name: set SELinux to enforcing hosts: localhost become: yes collections: - ansible.posix tasks: - name: set SElinux to enforcing selinux: policy: targeted state: enforcing  Although the syntax looks similar to how you specify roles, this works different. They keyword roles will execute the tasks/main.yml in each role. The collections keyword is merely a shortcut so you can skip the author and namespace every time you use a module in a task.\n Test the change Now run the Playbook again, you shouldn\u0026rsquo;t see any difference in the output. As explained before, the collections keyword only simplifies writing your Playbook!\nWe are explaining the collections keyword here for completeness. It is however recommended to always use the fully qualified collection name. The internal lookup can deliver unexpected results if there are many overlapping or overriding module names, which can be avoided by always using the full name. Since most modern code editors provide auto completion, it\u0026rsquo;s not too much of an issue when typing the code either.\n "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/2-clustering/",
	"title": "Introduction to automation controller clustering",
	"tags": [],
	"description": "",
	"content": "With version 3.1 Ansible Tower introduced clustering, replacing the redundancy solution configured with active-passive nodes. Clustering is balancing load between controller nodes. Each controller instance is able to act as an entry point for UI and API access.\nUsing a load balancer in front of the controller nodes is possible, but optional because an automation controller cluster can be accessed via all controller instances.\n Each instance in a controller cluster expands the cluster’s capacity to execute jobs. Jobs can and will run anywhere in the cluster by finding the least utilized node and other criteria explained later in this lab.\nThe Appendix contains some installation considerations and an installer inventory for reference.\n Access the Controller Web UI For the first contact to your cluster open your browser and login to the controller node 1 web UIs as user adminwith the password provided.\nReplace \u0026lt;SANDBOXID\u0026gt; and \u0026lt;GUID\u0026gt; with your values!\n https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\nJust from the web UI you wouldn’t know you’ve got a controller cluster at your hands here. To learn more about your cluster and its state, navigate to Administration -\u0026gt; Instance Groups. Here you will get an overview of the cluster by instance groups. Explore the information provided, of course there is no capacity used yet and no Jobs have run.\nRight now we have only one instance group named controlplane. When you get more groups, from this view you will see how the instances are distributed over the groups. Click on controlplane.\nTo dig deeper click on the Instances tab to get more information about the instances assigned to a group. In the instances view you can toggle nodes off/online and adjust the number of forks (don\u0026rsquo;t do this now). You’ll learn more about this later.\nAccess your controller cluster via command line You can also get information about your cluster on the command line. Log in to your VS Code again if you closed it by opening this URL in your browser:\nhttps://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com\nYour VSCode session is running on your bastion host. Again if not still open, open a terminal by clicking Terminal-\u0026gt;New Terminal in the menu bar.\nA terminal window opens at the bottom, become root:\n[lab-user@bastion ~]$ ssh autoctl1.\u0026lt;GUID\u0026gt;.internal In the terminal run the following command:\nYour exact hostnames will differ, of course!\n [ec2-user@autoctl1 ~]# sudo -i [root@autoctl1 ~]# awx-manage list_instances [controlplane capacity=171] autoctl1.\u0026lt;GUID\u0026gt;.internal capacity=57 version=4.0.0 heartbeat=\u0026#34;2020-08-27 09:06:21\u0026#34; autoctl2.\u0026lt;GUID\u0026gt;.internal capacity=57 version=4.0.0 heartbeat=\u0026#34;2020-08-27 09:05:58\u0026#34; autoctl3.\u0026lt;GUID\u0026gt;.internal capacity=57 version=4.0.0 heartbeat=\u0026#34;2020-08-27 09:06:00\u0026#34; So what we’ve got is a three-node controller cluster, no surprises here. In addition the command tells us the capacity (maximum number of forks/concurrent jobs) per node and for the instance groups. Here the capacity value of 57 is allocated to any of our three nodes.\nPlease logout of the automation controller node so in the terminal you are user lab-user on the bastion host again!\n The awx-manage (formerly tower-manage) utility can be used to manage a lot of the more internal aspects of the automation controller. You can e.g. use it to clean up old data, for token and session management and for cluster management.\n "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/2-cred/",
	"title": "Inventories, credentials and ad hoc commands",
	"tags": [],
	"description": "",
	"content": "Create an Inventory Let’s get started: The first thing we need is an inventory of your managed hosts. This is the equivalent of an inventory file when using Ansible on the command line. There is a lot more to it (like dynamic inventories) but let’s start with the basics.\n You should already have the web UI open, if not: Point your browser to the URL you were given, similar to https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com and log in as admin with the password provided.  Create the inventory:\n  In the web UI menu on the left side, go to Resources → Inventories, click the drop-down and choose Add Inventory.\n  Name: Workshop Inventory\n  Organization: Default\n  Click Save\n  Go back to the Inventories list, your new Workshop Inventory should show up. Open the Workshop Inventory and click the Hosts tab, the list will be empty since we have not added any hosts yet.\nSo let\u0026rsquo;s add some hosts. As mentioned in the intro you have three managed hosts in your lab environment. The nodes are named node1.\u0026lt;GUID\u0026gt;.internal, node2.\u0026lt;GUID\u0026gt;.internal and node3.\u0026lt;GUID\u0026gt;.internal, just replace \u0026lt;GUID\u0026gt; by your code.\nNow add the hosts to the inventory in automation controller:\n  Click the blue button.\n  Name: node1.\u0026lt;GUID\u0026gt;.internal\n  Click Save\n  Click the Back to Hosts button and repeat to add node2.\u0026lt;GUID\u0026gt;.internal as a second and node3.\u0026lt;GUID\u0026gt;.internal as a third node.\n  You have now created an inventory with three managed hosts.\nMachine Credentials One of the great features of automation controller is to make credentials usable to users without making them visible. To allow automation controller to execute jobs on remote hosts, you must configure connection credentials.\nThis is one of the most important features of automation controller: Credential Separation! Credentials are defined separately and not with the hosts or inventory settings.\n As this is an important part of your automation controller setup, why not make sure that connecting to the managed nodes from automation controller is working in the first place?\nTo test access to the nodes via SSH do the following:\n  In your browser bring up the terminal window in VS Code server (remember this runs on the bastion node).\n  From here (you should be lab-user) SSH into node1.\u0026lt;GUID\u0026gt;.internal or one of the other nodes and execute sudo -i.\n  In your lab environment SSH key authentication has already been configured.\n [lab-user@bastion ~]$ ssh node1.\u0026lt;GUID\u0026gt;.internal [ec2-user@node1 ~]$ sudo -i [root@node1 ~]# exit [ec2-user@node1 ~]$ exit What does this mean?\n  Automation controller user lab-user on the bastion node can connect to the managed hosts with SSH key authentication as user ec2-user (this happens automatically through ssh_config).\n  User ec2-user can execute commands on the managed hosts as root with sudo.\n  Configure Machine Credentials Now we will configure the credentials to access our managed hosts in automation controller. In the automation controller web ui go to the Resources menu and choose Credentials. Now:\nClick the button to add new credentials\n  Name: Workshop Credentials\n  Organization: Click on the magnifying glass, pick Default and click Select\n  Credential Type: Open the drop-down menu, and pick the type Machine\n  Username: ec2-user\n  Privilege Escalation Method: sudo\n  As we are using SSH key authentication, you have to provide an SSH private key that can be used to access the hosts. You could also configure password authentication here.\nBring up your VSCode terminal and cat the SSH private key of user lab-user (note your \u0026lt;GUID\u0026gt; is part of the key name, you have to replace it!):\n[lab-user@bastion ~]$ cat .ssh/\u0026lt;GUID\u0026gt;key.pem -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEA2nnL3m5sKvoSy37OZ8DQCTjTIPVmCJt/M02KgDt53+baYAFu1TIkC3Yk+HK1 [...] -----END RSA PRIVATE KEY-----   Copy the complete private key (including \u0026ldquo;BEGIN\u0026rdquo; and \u0026ldquo;END\u0026rdquo; lines) and paste it into the SSH Private Key field in the web UI.\n  Click Save\n  Your new credentials have been created, go back to the Resources -\u0026gt; Credentials -\u0026gt; Workshop Credentials, click Edit and note that the SSH key is not visible but shown as \u0026ldquo;encrypted\u0026rdquo;! Click Cancel to leave edit mode again.\nYou have now setup credentials for Ansible to access your managed hosts.\nHold on: Ansible Execution Environments Primer! Before you run your first Ansible ad hoc command in automation controller it\u0026rsquo;s about time to learn about one of the major new features in Ansible Automation Platform 2: Execution Environments!\nBefore AAP 2 the Automation Platform execution relied on using bubblewrap to isolate processes and Python virtual environments (venv) to sandbox dependencies. This lead to a number of issues like maintaining multiple venv, migrating Ansible content between execution nodes and much more. The concept of execution environments (EE) solves this by using Linux containers.\nAn EE is a container run from an image that contains everything your Ansible Playbook needs to run. It\u0026rsquo;s basically a control node in a box that can be executed everywhere a Linux container can run. There are ready-made images that contain everything you would expect on an Ansible control node, but you can (and probably will) start to build your own, custom image for your very own requirements at some point.\nYour automation controller has been preconfigured with some standard EE images. So first go through the next section covering ad hoc commands, we\u0026rsquo;ll look a bit deeper into execution environments later.\nLinux containers are technologies that allow you to package and isolate applications with their entire runtime environment. This makes it easy to move the contained application between environments and nodes while retaining full functionality. In this lab you\u0026rsquo;ll use the command podman later on. Podman is a daemon less container engine for developing, managing, and running Open Container Initiative (OCI) containers and container images on your Linux System. If you want to learn more, there is a wealth of information on the Internet, you could start here for Podman or here for execution environments.\n Run Ad Hoc Commands As you’ve probably done with Ansible before you can run ad hoc commands from automation controller as well.\n  In the web UI go to Resources → Inventories → Workshop Inventory\n  Click the Hosts button to change into the hosts view and select the three hosts by checking the boxes to the left of the host entries.\n  Click Run Command. In the next screen you have to specify the ad hoc command:\n  As Module choose ping\n  As the ping module doesn\u0026rsquo;t require arguments, click Next\n  As Execution Environment choose Default execution environment (Note: if you don\u0026rsquo;t specify an EE here, the Default EE will be used)\n  Click Next\n  For Machine Credential choose Workshop Credentials.\n  Click Launch, and watch the output. It should report SUCCESS for all nodes, of course.\n    The first run of a job using an execution environment (EE) can take a bit longer then subsequent runs if the EE image has to be pulled down from the container registry first.\n The simple ping module doesn’t need options. For other modules you need to supply the command to run as an argument. Run another ad hoc command, this time try the command module to find the user ID of the executing user using an ad hoc command.\n  Module: command\n  Arguments: id\n  How about trying to get some more private information from the system? Try to print out /etc/shadow.\n  Module: command\n  Arguments: cat /etc/shadow\n  Expect an error!\n Oops, the last one didn’t went well, all red.\nRe-run the last ad hoc command but this time tick the Enable Privilege Escalation box.\nAs you see, this time it worked. For tasks that have to run as root you need to escalate the privileges. This is the same as the become: yes you’ve probably used often in your Ansible Playbooks.\nChallenge Lab: Ad Hoc Commands Okay, a small challenge: Run an ad hoc to make sure the package \u0026ldquo;nano\u0026rdquo; is installed on all hosts. If unsure, consult the Ansible documentation either via the web or by running ansible-doc yum in the VS Code terminal on your automation controller control host.\nClick here for Solution     Module: yum\n  Arguments: name=nano\n  Tick Enable Privilege Escalation\n    The yellow output of the command indicates Ansible has actually done something (here it needed to install the package). If you run the ad hoc command a second time, the output will be green and inform you that the package was already installed. So yellow in Ansible doesn’t mean \u0026ldquo;be careful\u0026rdquo;…​ ;-).\n Try to click one of the output lines in the window showing the job output. A small window Host Details will open. Clock the JSON \u0026ldquo;tab\u0026rdquo; of the window and have a look at the information you\u0026rsquo;ll get.\n Execution Environments: A deeper look As promised let\u0026rsquo;s look a bit deeper into execution environments. During the section covering ad hoc commands you have already seen you can choose an execution environment (or get the default), the same will hold true for running Playbooks later on. In your automation controller web UI, go to Administration → Execution Environments. You\u0026rsquo;ll see a list of the configured execution environments and original location of the image, in our case the images are provided in the quay.io container registry. Here you could add your own registry with custom EE images, too.\nSo what happens, when automation controller runs an ad hoc command or Playbook? Let\u0026rsquo;s see\u0026hellip;\nYou should already have your VS Code terminal open in another browser tab, if not open https://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com and do Terminal -\u0026gt; New Terminal. In this terminal:\n  SSH into your automation controller node (obviously replace \u0026lt;GUID\u0026gt; by your value):\n ssh autoctl1.\u0026lt;GUID\u0026gt;.internal    You should be the ec2-user user on your automation controller now, become the awx user by running sudo -u awx -i\n  First let\u0026rsquo;s look for the image, you should have used the Default execution environment during the ad-hoc command exercise which should result in this image (details might be different):\n  [awx@autoctl1 ~]$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE registry.redhat.io/ansible-automation-platform-20-early-access/ee-supported-rhel8 2.0.0 85ca2003a842 5 weeks ago 920 MB   It was pulled from the registry to be available locally.\n  Now we want to observe how an EE is run as a container when you execute your automation. podman ps -w 2 will list running containers, the -w option updates the output regularly. Run the command, at first it will show you something like this (no container running):\n  CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES   Keep podman running, now it\u0026rsquo;s time to execute some automation.\n  In the web UI run an ad hoc command again. Go to Resources → Inventories → Workshop Inventory\n  In the Hosts view select the three hosts.\n  Click Run Command. Specify the ad hoc command as you did before:\n  Module: command\n  Arguments: sleep 60\n  Next\n  Execution Environment: Don\u0026rsquo;t specify an EE, the Default execution environment will be used.\n  Next\n  Machine Credential: Workshop Credentials\n  Click Launch\n    Now go back to the VS Code terminal. You\u0026rsquo;ll see a container is launched to run the ad hoc command and then removed again:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c8e9a13ab475 quay.io/ansible/awx-ee:0.2.0 ssh-agent sh -c t... 1 second ago Up 2 seconds ago ansible_runner_18  The sleep 60 command was only used to keep the container running for some time. Your output will differ slightly.\n   Feel free to relaunch the job again!\n  Stop podman with CTRL-C.\n  Logout of automation controller (exit, exit)\n  This is how automation controller uses Linux containers to run Ansible automation jobs in their own dedicated environments.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/2-adhoc/",
	"title": "Running ad hoc commands",
	"tags": [],
	"description": "",
	"content": "For our first exercise, we are going to run some ad hoc commands to help you get a feel for how Ansible works. Ansible ad hoc commands enable you to perform tasks on remote nodes without having to write a playbook. They are very useful when you simply need to do one or two things quickly and often, to many remote nodes.\nWork with your Inventory To use the ansible command for host management, you need to provide an inventory file which defines a list of hosts to be managed from the control node. In this lab the inventory is provided by your instructor. The inventory is an ini formatted file listing your hosts, sorted in groups, additionally providing some variables. Have a look for yourself, in your VS Code server terminal run:\n[ec2-user@autoctl1 ~]$ cat lab_inventory/hosts [all:vars] ansible_user=student\u0026lt;GUID\u0026gt; ansible_ssh_pass=\u0026lt;password\u0026gt; ansible_port=22 [web] node1 ansible_host=\u0026lt;IP address\u0026gt; node2 ansible_host=\u0026lt;IP address\u0026gt; node3 ansible_host=\u0026lt;IP address\u0026gt; [control] ansible ansible_host=\u0026lt;IP address\u0026gt;  The environment for this lab uses SSH with password authentication to login to the managed nodes. For the sake of keeping things simple the password is put into the inventory file in clear text. In real world scenarios you would either use SSH key authentication or supply the password in a secure way, e.g. by using Ansible Vault.\n Note that each student has an individual lab environment so we left out the actual IPs and other data. As with the other cases, replace \u0026lt;N\u0026gt; with your actual student number.\n Ansible is already configured to use the inventory specific to your environment, you\u0026rsquo;ll learn how in a minute. For now let\u0026rsquo;s execute some simple commands to work with the inventory.\nTo reference inventory hosts, you supply a host pattern to the ansible command. Ansible has a --list-hosts option which can be useful for clarifying which managed hosts are referenced by the host pattern in an ansible command.\nThe most basic host pattern is the name for a single managed host listed in the inventory file. This specifies that the host will be the only one in the inventory file that will be acted upon by the ansible command. Run:\n[ec2-user@autoctl1 ~]$ ansible node1 --list-hosts hosts (1): node1 An inventory file can contain a lot more information, it can organize your hosts in groups or define variables. In our example, the current inventory has the groups web and control. Run Ansible with these host patterns and observe the output:\n[ec2-user@autoctl1 ~]$ ansible web --list-hosts [ec2-user@autoctl1 ~]$ ansible web,ansible --list-hosts [ec2-user@autoctl1 ~]$ ansible \u0026#39;node*\u0026#39; --list-hosts [ec2-user@autoctl1 ~]$ ansible all --list-hosts As you see it is OK to put systems in more than one group. For instance, a server could be both a web server and a database server. Note that in Ansible the groups are not necessarily hierarchical.\nThe inventory can contain more data. E.g. if you have hosts that run on non-standard SSH ports you can put the port number after the hostname with a colon. Or you could define names specific to Ansible and have them point to the \u0026ldquo;real\u0026rdquo; IP or hostname.\n The Ansible Configuration Files The behavior of Ansible can be customized by modifying settings in Ansible’s ini-style configuration file. Ansible will select its configuration file from one of several possible locations on the control node, please refer to the documentation.\nThe recommended practice is to create an ansible.cfg file in the directory from which you run Ansible commands. This directory would also contain any files used by your Ansible project, such as the inventory and playbooks. Another recommended practice is to create a file .ansible.cfg in your home directory.\n In the lab environment provided to you an .ansible.cfg file has already been created and filled with the necessary details in the home directory of your student\u0026lt;GUID\u0026gt; user on the control node:\n[ec2-user@autoctl1 ~]$ ls -la .ansible.cfg -rw-r--r--. 1 student\u0026lt;GUID\u0026gt; student\u0026lt;GUID\u0026gt; 231 14. Mai 17:17 .ansible.cfg Review the content of the file:\n[ec2-user@autoctl1 ~]$ cat .ansible.cfg [defaults] stdout_callback = yaml connection = smart timeout = 60 deprecation_warnings = False host_key_checking = False retry_files_enabled = False inventory = /home/student{{student}}/lab_inventory/hosts There are multiple configuration flags provided. Most of them are not of interest here, but make sure to note the last line: there the location of the inventory is provided. That is the way Ansible knew in the previous commands what inventory to use to lookup the nodes.\nPing a host Let\u0026rsquo;s start with something really basic - pinging a host. To do that we use the Ansible ping module. Basically, the ping module connects to the managed host, executes a small script there and collects the results. This ensures that the managed host is reachable and that Ansible is able to execute commands properly on it.\nThink of a module as a tool which is designed to accomplish a specific task.\n Ansible needs to know that it should use the ping module: The -m option defines which Ansible module to use. Options can be passed to the specified module using the -a option. In addition to the module Ansible needs to know what hosts it should run the task on, here you supply the group web.\n[ec2-user@autoctl1 ~]$ ansible web -m ping node2 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } [...] As you see each node in the web group reports the successful execution and the actual result - here \u0026ldquo;pong\u0026rdquo;.\nListing Modules and Getting Help Ansible comes with a lot of modules by default. To list all modules run:\n[ec2-user@autoctl1 ~]$ ansible-doc -l  In ansible-doc leave by pressing the button q. Use the up/down arrows to scroll through the content.\n To find a module try e.g.:\n[ec2-user@autoctl1 ~]$ ansible-doc -l | grep -i user Get help for a specific module including usage examples:\n[ec2-user@autoctl1 ~]$ ansible-doc user  Mandatory options are marked by a \u0026ldquo;=\u0026rdquo; in ansible-doc, optional ones by a \u0026ldquo;-\u0026quot;.\n Use the command module Now let\u0026rsquo;s see how we can run a good ol\u0026rsquo; fashioned Linux command and format the output using the command module. It simply executes the specified command on a managed host (note this time not a group but a hostname is used as host pattern):\n[ec2-user@autoctl1 ~]$ ansible node1 -m command -a \u0026#34;id\u0026#34; node1 | CHANGED | rc=0 \u0026gt;\u0026gt; uid=1001(student\u0026lt;GUID\u0026gt;) gid=1001(student{{ student }) groups=1001(student\u0026lt;GUID\u0026gt;) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 In this case the module is called command and the option passed with -a is the actual command to run. Try to run this ad hoc command on all managed hosts using the all host pattern.\nAnother example: Have a quick look at the kernel versions your hosts are running:\n[ec2-user@autoctl1 ~]$ ansible all -m command -a \u0026#39;uname -r\u0026#39; Sometimes it’s desirable to have the output for a host on one line:\n[ec2-user@autoctl1 ~]$ ansible all -m command -a \u0026#39;uname -r\u0026#39; -o  Like many Linux commands, ansible allows long-form options as well as short-form. For example ansible web --module-name ping is the same as running ansible web -m ping. We are going to be using the short-form options throughout this workshop.\n The copy module and permissions Using the copy module, execute an ad hoc command on node1 to change the contents of the /etc/motd file. The content is handed to the module through an option in this case.\nRun the following, but expect an error:\n[ec2-user@autoctl1 ~]$ ansible node1 -m copy -a \u0026#39;content=\u0026#34;Managed by Ansible\\n\u0026#34; dest=/etc/motd\u0026#39; As mentioned this produces an error:\nnode1 | FAILED! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;checksum\u0026#34;: \u0026#34;a314620457effe3a1db7e02eacd2b3fe8a8badca\u0026#34;, \u0026#34;failed\u0026#34;: true, \u0026#34;msg\u0026#34;: \u0026#34;Destination /etc not writable\u0026#34; } The output of the ad hoc command is screaming FAILED in red at you. Why? Because user student\u0026lt;GUID\u0026gt; is not allowed to write the motd file.\nNow this is a case for privilege escalation and the reason sudo has to be setup properly. We need to instruct Ansible to use sudo to run the command as root by using the parameter -b (think \u0026ldquo;become\u0026rdquo;).\nAnsible will connect to the machines using your current user name (student\u0026lt;GUID\u0026gt; in this case), just like SSH would. To override the remote user name, you could use the -u parameter.\n For us it’s okay to connect as student\u0026lt;GUID\u0026gt; because sudo is set up. Change the command to use the -b parameter and run again:\n[ec2-user@autoctl1 ~]$ ansible node1 -m copy -a \u0026#39;content=\u0026#34;Managed by Ansible\\n\u0026#34; dest=/etc/motd\u0026#39; -b This time the command is a success:\nnode1 | CHANGED =\u0026gt; { \u0026#34;changed\u0026#34;: true, \u0026#34;checksum\u0026#34;: \u0026#34;4458b979ede3c332f8f2128385df4ba305e58c27\u0026#34;, \u0026#34;dest\u0026#34;: \u0026#34;/etc/motd\u0026#34;, \u0026#34;gid\u0026#34;: 0, \u0026#34;group\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;md5sum\u0026#34;: \u0026#34;65a4290ee5559756ad04e558b0e0c4e3\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;0644\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;secontext\u0026#34;: \u0026#34;system_u:object_r:etc_t:s0\u0026#34;, \u0026#34;size\u0026#34;: 19, \u0026#34;src\u0026#34;: \u0026#34;/home/student\u0026lt;GUID\u0026gt;/.ansible/tmp/ansible-tmp-1557857641.21-120920996103312/source\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;uid\u0026#34;: 0 Use Ansible with the generic command module to check the content of the motd file:\n[ec2-user@autoctl1 ~]$ ansible node1 -m command -a \u0026#39;cat /etc/motd\u0026#39; node1 | CHANGED | rc=0 \u0026gt;\u0026gt; Managed by Ansible Run the ansible node1 -m copy …​ command from above again. Note:\n  The different output color (proper terminal config provided).\n  The change from \u0026quot;changed\u0026quot;: true, to \u0026quot;changed\u0026quot;: false,.\n  The first line says SUCCESS instead of CHANGED.\n  This makes it a lot easier to spot changes and what Ansible actually did.\n Challenge Lab: Modules   Using ansible-doc\n  Find a module that uses yum to manage software packages.\n  Look up the help examples for the module to learn how to install a package in the latest version.\n    Run an Ansible ad hoc command to install the package \u0026ldquo;vim\u0026rdquo; in the latest version on all available nodes.\n  Use the copy ad hoc command from above as a template and change the module and options.\n Click here for Solution   [ec2-user@autoctl1 ~]$ ansible-doc -l | grep -i yum [ec2-user@autoctl1 ~]$ ansible-doc yum [ec2-user@autoctl1 ~]$ ansible all -m yum -a \u0026#39;name=vim state=latest\u0026#39; -b \n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/",
	"title": "Ansible automation controller advanced",
	"tags": [],
	"description": "",
	"content": "Ansible automation controller Advanced   Exercise 1 - Discover your lab\n  Exercise 2 - Introduction to automation controller clustering\n  Exercise 3 - There is more to automation controller than the web UI\n  Exercise 4 - Run a job in a cluster\n  Exercise 5 - Controller instance groups\n  Exercise 6 - Start parallel jobs across instances\n  Exercise 7 - Advanced inventories\n  Exercise 8 - OPTIONAL: Well structured content repositories\n  Exercise 9 - OPTIONAL: Discovering the automation controller API\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/3-using-collections-from-roles/",
	"title": "Collections from Roles",
	"tags": [],
	"description": "",
	"content": "In this exercise you will learn how collections are used from within Ansible roles. As promised we\u0026rsquo;ll explain the collection name resolution logic in more depth first. Then you\u0026rsquo;ll see how to call a collection from an Ansible role using collection fully qualified collection name (FQCN).\nRunning collections from an Ansible role First and to keep you home directory tidy, create an exercise folder:\n[ec2-user@autoctl1 ~]$ mkdir exercise-03 [ec2-user@autoctl1 ~]$ cd exercise-03 For this lab, we will use the ansible.posix collection again, which contains a series of POSIX oriented modules and plugins for systems management. You should have installed the collection in the first exercise, if not, just do it now:\n[ec2-user@autoctl1 ~]$ ansible-galaxy collection install ansible.posix Process install dependency map Starting collection install process Skipping \u0026#39;ansible.posix\u0026#39; as it is already installed  See how the command didn\u0026rsquo;t fail or complain if the collection was installed already but just let\u0026rsquo;s you know it was already there.\n Approach 1: Collections loaded as metadata There are two approaches to use Ansible Collections in your role. The first approach is to specify the collection you want to use in your roles metadata.\nFirst we\u0026rsquo;ll create a simple role. Start with creating a new role scaffold using the ansible-galaxy init command (make sure you changed into your exercise folder):\n[ec2-user@autoctl1 exercise-03]$ ansible-galaxy init --init-path roles selinux_manage_meta - Role selinux_manage_meta was created successfully Now you have to edit a couple of files. First edit the role metadata in roles/selinux_manage_meta/meta/main.yml and append the following lines at the end of the file, don\u0026rsquo;t change anything else:\n# Collections list collections: - ansible.posix A role without some tasks is not too interesting, go and edit the roles/selinux_manage_meta/tasks/main.yml file and add the following tasks (make sure to keep the YAML start --- in place):\n--- # tasks file for selinux_manage_meta - name: Enable SELinux enforcing mode selinux: policy: targeted state: \u0026quot;{{ selinux_mode }}\u0026quot; - name: Enable booleans seboolean: name: \u0026quot;{{ item }}\u0026quot; state: true persistent: true loop: \u0026quot;{{ sebooleans_enable }}\u0026quot; - name: Disable booleans seboolean: name: \u0026quot;{{ item }}\u0026quot; state: false persistent: true loop: \u0026quot;{{ sebooleans_disable }}\u0026quot;  We\u0026rsquo;re using the simple module name. Ansible uses the information from the collections list in the metadata file to locate the collection(s) used.\n Every well-written role should come with sensible defaults, edit the roles/selinux_manage_meta/defaults/main.yml to define default values for role variables:\n--- # defaults file for selinux_manage_meta selinux_mode: enforcing sebooleans_enable: [] sebooleans_disable: [] As a last step clean up the unused folders of the role:\n[ec2-user@autoctl1 exercise-03]$ rm -rf roles/selinux_manage_meta/{tests,vars,handlers,files,templates} And you are done with the role! Run tree to check everything looks good:\n[ec2-user@autoctl1 exercise-03]$ tree . └── roles └── selinux_manage_meta ├── defaults │ └── main.yml ├── meta │ └── main.yml ├── README.md └── tasks └── main.yml We can now test the new role with a basic playbook. Create the playbook.yml file in the exercise folder with the following content:\n--- - hosts: localhost become: true vars: sebooleans_enable: - httpd_can_network_connect - httpd_mod_auth_pam sebooleans_disable: - httpd_enable_cgi roles: - selinux_manage_meta Run the playbook and check the results:\n[ec2-user@autoctl1 exercise-03]$ ansible-playbook playbook.yml [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match \u0026#39;all\u0026#39; PLAY [localhost] ****************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************ ok: [localhost] TASK [selinux_manage_meta : Enable SELinux enforcing mode] ************************************************************ ok: [localhost] TASK [selinux_manage_meta : Enable booleans] ************************************************************************** changed: [localhost] =\u0026gt; (item=httpd_can_network_connect) changed: [localhost] =\u0026gt; (item=httpd_mod_auth_pam) TASK [selinux_manage_meta : Disable booleans] ************************************************************************* changed: [localhost] =\u0026gt; (item=httpd_can_network_connect) PLAY RECAP ************************************************************************************************************ localhost : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Approach 2: Collections loaded with FQCN The second approach to use collections in roles uses the collection FQCN to call the related modules and plugins. To better demonstrate the differences, we will implement a new version of the previous role with the FQCN approach without changing the inner logic.\nTo make this easy we\u0026rsquo;ll just copy and edit the role we created above:\n[ec2-user@autoctl1 exercise-03]$ cp -r roles/selinux_manage_meta/ roles/selinux_manage_fqcn Now change the file roles/selinux_manage_fqcn/tasks/main.yml so it uses the FQCN for the modules. It should look like this:\n--- # tasks file for selinux_manage_fqcn - name: Enable SELinux enforcing mode ansible.posix.selinux: policy: targeted state: \u0026quot;{{ selinux_mode }}\u0026quot; - name: Enable booleans ansible.posix.seboolean: name: \u0026quot;{{ item }}\u0026quot; state: true persistent: true loop: \u0026quot;{{ sebooleans_enable }}\u0026quot; - name: Disable booleans ansible.posix.seboolean: name: \u0026quot;{{ item }}\u0026quot; state: false persistent: true loop: \u0026quot;{{ sebooleans_disable }}\u0026quot;  Notice the usage of the modules FQCN in the role tasks. Ansible will directly look for the installed collection from within the role task, no matter if the collections keyword is defined at playbook level.\n The remaining files can stay unchanged. To test the FQCN role modify the previously created playbook.yml file to use the new role:\n--- - hosts: localhost become: true vars: sebooleans_enable: - httpd_can_network_connect - httpd_mod_auth_pam sebooleans_disable: - httpd_enable_cgi roles: - selinux_manage_fqcn Run the playbook again and check the results:\n[ec2-user@autoctl1 exercise-03]$ ansible-playbook playbook.yml [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match \u0026#39;all\u0026#39; PLAY [localhost] ****************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************ ok: [localhost] TASK [selinux_manage_meta : Enable SELinux enforcing mode] ************************************************************ ok: [localhost] TASK [selinux_manage_meta : Enable booleans] ************************************************************************** changed: [localhost] =\u0026gt; (item=httpd_can_network_connect) ok: [localhost] =\u0026gt; (item=httpd_mod_auth_pam) TASK [selinux_manage_meta : Disable booleans] ************************************************************************* changed: [localhost] =\u0026gt; (item=httpd_can_network_connect) PLAY RECAP ************************************************************************************************************ localhost : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 The Playbook should run in the same way with the same output like above, where you loaded the collection using the role metadata.\nTakeaways   Collections can be called from roles using the collections list defined in meta/main.yml.\n  Collections can be called from roles using their FQCN directly from the role tasks.\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/3-projects/",
	"title": "Projects &amp; job templates",
	"tags": [],
	"description": "",
	"content": "An automation controller Project is a logical collection of Ansible Playbooks. You manage your playbooks by placing them into a source code management (SCM) system supported by automation controller, including Git, Subversion, and others.\nAnd yes, you should definitely keep your Playbooks under version control.\nSetup a Git Repository as a Project In this lab we’ll use existing Playbooks that are provided in this Github repository:\nhttps://github.com/ansible-labs-crew/playbooks_summit2021\nA Playbook to install the Apache webserver has already been committed to the directory rhel/apache, apache_install.yml, here for reference:\n--- - name: Apache server installed hosts: all tasks: - name: latest Apache version installed yum: name: httpd state: latest - name: latest firewalld version installed yum: name: firewalld state: latest - name: firewalld enabled and running service: name: firewalld enabled: true state: started - name: firewalld permits http service firewalld: service: http permanent: true state: enabled immediate: yes - name: Apache enabled and running service: name: httpd enabled: true state: started  Note the difference to other Playbooks you might have written! Most importantly there is no become and hosts is set to all.\n To configure and use this repository as a Source Code Management (SCM) system in automation controller you have to create a Project that uses the repository\nCreate the Project   Go to Resources → Projects in the side menu view click the button. Fill in the form:\n  Name: Ansible Workshop Examples\n  Organization: Default\n  Source Control Credential Type: Git\n  Now you need the URL to access the repo. You could get the URL in Github as Clone URL. Enter the URL into the Project configuration:\n  Source Control URL: https://github.com/ansible-labs-crew/playbooks_summit2021.git\n  Options: Tick the boxes Clean, Delete, Update Revision on Launch to always get a fresh copy of the repository and to update the repository when launching a job.\n  Click Save\n  The new Project will be synced automatically after creation. But you can also do this manually: Sync the Project again with the Git repository by going to the Projects view and clicking the circular arrow Sync Project icon to the right of the Project.\nAfter starting the sync job, go to the Jobs view: there are new jobs for the update of the Git repository.\nCreate a Job Template and Run a Job A job template is a definition and set of parameters for running an Ansible job. Job templates are useful to execute the same job many times. So before running an Ansible Job from automation controller you must create a Job Template that pulls together:\n  Inventory: On what hosts should the job run?\n  Credentials What credentials are needed to log into the hosts?\n  Project: Where is the Playbook?\n  What Playbook to use?\n  Okay, let’s just do that: Go to the Templates view, click the button and choose Add job template.\nRemember that you can often click on magnifying glasses to get an overview of options to pick to fill in fields.\n   Name: Install Apache\n  Job Type: Run\n  Inventory: Workshop Inventory\n  Project: Ansible Workshop Examples\n  Execution Environment: Default execution environment\n  Playbook: rhel/apache/apache_install.yml\n  Credentials: Workshop Credentials\n  We need to run the tasks as root so check Privilege Escalation under Options\n  Click Save\n  You can start the job directly from the Details view by clicking the Launch button, or by clicking on the rocket in the Templates overview. After launching the Job Template, you are automatically brought to the job overview where you can follow the playbook execution in real time:\nSince this might take some time, have a closer look at all the details provided:\n The default is the Output view of the playbook run. Click on a node underneath a task and switch to JSON to get detailed information for the task.  Now switch to the Details view:\n  All details of the job template like inventory, project, credentials and playbook are shown.\n  Additionally, the actual revision of the playbook is recorded here - this makes it easier to analyse job runs later on.\n  Also the time of execution with start and end time is recorded, giving you an idea of how long a job execution actually was.\n  After the Job has finished go to the main Jobs view: All jobs are listed here, you should see directly before the Playbook run a job of type Source Control Update was started. This is the Git update we configured for the Project on Template launch!\nChallenge Lab: Check the Result Time for a little challenge:\n Use an ad hoc command on all hosts to make sure Apache has been installed and is running.  You have already been through all the steps needed, so try this for yourself.\nWhat about systemctl status httpd?\n Click here for Solution     Go to Inventories → Workshop Inventory\n  On the Hosts tab view select all hosts and click Run Command\n  Module: command\n  Arguments: systemctl status httpd\n  Next\n  Execution Environment: Don\u0026rsquo;t specify, the Default execution environment will be used.\n  Next\n  Machine Credential: Workshop Credentials\n  Click Launch\n    It should show Apache running on all nodes!\nWhat About Some Practice? Here is a list of tasks:\nPlease make sure to finish these steps as the next chapter depends on it!\n   Create a new inventory called Webserver and make only node1.\u0026lt;GUID\u0026gt;.internal member of it.\n  Copy the Install Apache template using the copy icon in the Templates view.\n  Edit the Template and change the name to Install Apache Ask.\n  Change the Inventory setting of the Project so it will prompt for the inventory on launch (tick the appropriate box).\n  Save\n  Launch the Install Apache Ask template.\n  It will now ask for the inventory to use, choose the Webserver inventory and click Next and Launch.\n  Wait until the Job has finished and make sure it ran only on node1.\u0026lt;GUID\u0026gt;.internal.\n  The Job didn’t change anything because Apache was already installed in the latest version.\n "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/3-awx-collection-intro/",
	"title": "There is more to automation controller than the Web UI",
	"tags": [],
	"description": "",
	"content": "This is an advanced automation controller lab so we don’t really want you to use the web UI for everything. To fully embrace automation and adopt the infrastructure as code methodology, we want to use Ansible to configure our automation controller cluster.\nSince automation controller is exposing all of its functionality via REST API, we can automate everything. Instead of using the API directly, it is highly recommended to use the AWX or automation controller Ansible Collection (the second link will only work for you, if you have an active Red Hat Ansible Automation Platform Subscription) to setup, configure and maintain your Red Hat Ansible Automation Platform Cluster.\nFor the purpose of this lab, we will use the community AWX collection. Red Hat Customers will prefer the supported Ansible automation controller collection. Since this requires an active subscription and we want to make the lab usable for everyone, we will stick with the AWX collection for the purpose of the lab.\n First, we need to install the AWX Collection on the bastion node. Installing Ansible Collections is straight forward:\n[lab-user@bastion ~]$ ansible-galaxy collection install awx.awx:19.2.2  The AWX collection is updated very often. To make sure the following lab instructions will work for you, we install specifically version 19.2.2. We regularly update these instruction to keep up to date and don\u0026rsquo;t fall behind too much. However, to make sure the lab works for you, we suggest you to use the specified version.\n Authentication Before we can do any changes on our automation controller, we have to authenticate our user. There are several methods available to provide authentication details to the modules. In this lab, we\u0026rsquo;ll use environment variables. In your VSCode UI use either your preferred editor from the command line or the visual editor (open a new file by going to File-\u0026gt;New File) to open a new file and add the following:\n# the Base URI of our automation controller cluster node export CONTROLLER_HOST=https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com # the user name export CONTROLLER_USERNAME=admin # and the password export CONTROLLER_PASSWORD=\u0026#39;VERY_SECRET_PASSWORD\u0026#39; # do not verify the SSL certificate, in production, you will use proper SSL certificates and not need this option or set it to True export CONTROLLER_VERIFY_SSL=false  As always make sure to replace \u0026lt;GUID\u0026gt; and \u0026lt;SANDBOXID\u0026gt;!\n Save the file in the home directory of lab-user as set-connection.sh, when using the visual editor in VSCode do File-\u0026gt;Save as.\nNow set the environment variables by sourcing the file:\n[lab-user@bastion ~]$ source set-connection.sh  Make sure you define the environment variables in the same shell you want to later run your Ansible Playbook from, otherwise the Playbook will fail due to authentication errors. If you lost connection to VSCode, just source the set-connection.sh file again.\n Create an inventory Now we\u0026rsquo;ll start to create a Playbook with the tasks needed to configure our automation controller. Let\u0026rsquo;s start with creating an inventory and then extend the Playbook step by step.\nIn your VSCode UI use either your preferred editor from the command line or the visual editor again to open a new file and add the following:\n--- - name: Configure automation controller hosts: localhost become: false gather_facts: false tasks: - name: Create an inventory awx.awx.inventory: name: AWX inventory organization: Default Since we are calling the REST API of automation controller, the Ansible Playbook is run against localhost, but the module will connect to the URL provided by the CONTROLLER_HOST environment variable you set above.\nSave the file in the home directory of lab-user as configure-controller.yml.\nRun your playbook:\n[lab-user@bastion ~]$ ansible-playbook configure-controller.yml Now verify everything worked as expected by logging into the automation controller Web UI and checking the inventory AWX Inventory has been created\nYou might see a warning message \u0026ldquo;You are using the awx version of this collection but connecting to Red Hat Ansible Automation Platform\u0026rdquo;. This can be ignored. As said above, we intentionally use the AWX Community Collection for the purpose of the lab. As a Red Hat Customer you would probably prefer the supported Ansible Collection instead.\n Add hosts to inventory Now that we have the empty inventory created, extend the Playbook to add your three managed hosts using their internal hostnames node1.\u0026lt;GUID\u0026gt;.internal and node2.\u0026lt;GUID\u0026gt;.internal and node3.\u0026lt;GUID\u0026gt;.internal, again using the AWX Ansible Collection. The module to add hosts to an inventory is called awx.awx.host.\n Read the documentation and try to figure out how to add the necessary tasks to the Ansible Playbook or use the solution below. Add the task to the Playbook in your favorite editor.  Use ansible-doc awx.awx.host to open the documentation for the specified module.\n Click here for Solution   --- - name: Configure automation controller hosts: localhost become: false gather_facts: false tasks: - name: Create an inventory awx.awx.inventory: name: AWX inventory organization: Default - name: Add hosts to inventory awx.awx.host: name: \u0026#34;{{ item }}\u0026#34; inventory: AWX inventory state: present loop: - node1.\u0026lt;GUID\u0026gt;.internal - node2.\u0026lt;GUID\u0026gt;.internal - node3.\u0026lt;GUID\u0026gt;.internal \n  Run your playbook again and verify everything worked as expected, by logging into the automation controller Web UI and verifying the hosts have been created in the inventory AWX Inventory.\nCreate Machine Credentials From here onwards we\u0026rsquo;ll extend the Playbook, it\u0026rsquo;s up to you if you add all sections to the Playbook first and then run it or if you do it step by step.\nSSH keys have already been created and distributed in your lab environment and sudo has been setup on the managed hosts to allow password-less login. When you SSH into a host as user lab-user from bastion.\u0026lt;GUID\u0026gt;.internal you will become user ec2-user on the host you logged in.\n The next step is to configure credentials to access our managed hosts. The private key for lab-user is stored in /home/lab-user/.ssh/\u0026lt;GUID\u0026gt;key.pem and already configured on the managed nodes. Try to find the necessary attributes in the awx.awx.credential module documentation or use the solution provided and add this to the Playbook.\nAnd again do not forget to replace \u0026lt;GUID\u0026gt;! We\u0026rsquo;ll stop mentioning this at some point, though\u0026hellip; :-)\n Click here for Solution   --- - name: Configure automation controller hosts: localhost become: false gather_facts: false tasks: - name: Create an inventory awx.awx.inventory: name: AWX inventory organization: Default - name: Add hosts to inventory awx.awx.host: name: \u0026#34;{{ item }}\u0026#34; inventory: AWX inventory state: present loop: - node1.\u0026lt;GUID\u0026gt;.internal - node2.\u0026lt;GUID\u0026gt;.internal - name: Machine Credentials awx.awx.credential: name: AWX Credentials kind: ssh organization: Default inputs: username: ec2-user ssh_key_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;~/.ssh/\u0026lt;GUID\u0026gt;key.pem\u0026#39; ) }}\u0026#34; \n  If you run this Ansible Playbook multiple times, you will notice the awx.awx.credential module is not idempotent! Since we store the SSH key encrypted, the Ansible Module is unable to verify it has already been set and didn\u0026rsquo;t change. This is what we want and expect from a secure system, but it also means Ansible has no means to verify it and hence overrides the SSH key or password every time the Ansible Playbook is executed.\n Run and test your playbook and verify everything works as expected, by logging into the automation controller Web UI.\nCreate Project The Ansible content used in this lab is hosted on Github in the project https://github.com/ansible-labs-crew/playbooks_adv_summit2021.git. The next step is to add a project to import the Ansible Playbooks. As before, try to figure out the necessary parameters by reading the documentation of the awx.awx.project module documentation.\nClick here for Solution   --- - name: Configure automation controller hosts: localhost become: false gather_facts: false tasks: - name: Create an inventory awx.awx.inventory: name: AWX inventory organization: Default - name: Add hosts to inventory awx.awx.host: name: \u0026#34;{{ item }}\u0026#34; inventory: AWX inventory state: present loop: - node1.\u0026lt;GUID\u0026gt;.internal - node2.\u0026lt;GUID\u0026gt;.internal - name: Machine Credentials awx.awx.credential: name: AWX Credentials kind: ssh organization: Default inputs: username: ec2-user ssh_key_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;~/.ssh/aws-private.pem\u0026#39; ) }}\u0026#34; - name: AWX Project awx.awx.project: name: AWX Project organization: Default state: present scm_update_on_launch: True scm_delete_on_update: True scm_type: git scm_url: https://github.com/ansible-labs-crew/playbooks_adv_summit2021.git \n  Run and test your playbook and verify everything works as expected, by logging into the automation controller Web UI.\nIf you wonder why awx.awx.credential is not idempotent, read the info box in the previous chapter.\nCreate a Template Before running an Ansible Job from your automation controller cluster you must create a Template, again business as usual for automation controller users. For this part of the Ansible Playbook, we will use the awx.awx.job_template module. The name of the Ansible Playbook you want run is apache_install.yml.\nClick here for Solution   --- - name: Configure automation controller hosts: localhost become: false gather_facts: false tasks: - name: Create an inventory awx.awx.inventory: name: AWX inventory organization: Default - name: Add hosts to inventory awx.awx.host: name: \u0026#34;{{ item }}\u0026#34; inventory: AWX inventory state: present loop: - node1.\u0026lt;GUID\u0026gt;.internal - node2.\u0026lt;GUID\u0026gt;.internal - name: Machine Credentials awx.awx.credential: name: AWX Credentials kind: ssh organization: Default inputs: username: ec2-user ssh_key_data: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;~/.ssh/aws-private.pem\u0026#39; ) }}\u0026#34; - name: AWX Project awx.awx.project: name: AWX Project organization: Default state: present scm_update_on_launch: True scm_delete_on_update: True scm_type: git scm_url: https://github.com/ansible-labs-crew/playbooks_adv_summit2021.git - name: AWX Job Template awx.awx.job_template: name: Install Apache organization: Default state: present inventory: AWX inventory become_enabled: True playbook: apache_install.yml project: AWX Project credential: AWX Credentials \n  Run and test your playbook and verify everything works as expected, by logging into the automation controller Web UI.\nVerify the cluster We are working in a clustered environment. To verify that the resources were created on all instances properly, login to the other controller nodes web UI\u0026rsquo;s.\nHave a look around, everything we automatically configured on one controller instance with our Ansible Playbook was synchronized automatically to the other nodes. Inventory, credentials, projects, templates, all there.\nChallenge Labs Add a task to the Ansible Playbook you wrote in this chapter to automatically run the job Template you created. If you don\u0026rsquo;t know how to start, check out the documentation of the awx.awx.job_launch module by running ansible-doc in your VSCode terminal:\n[lab-user@bastion ~]$ ansible-doc awx.awx.tower_job_launch Click here for Solution   This is a Challenge Lab! No solution here.\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/3-playbook/",
	"title": "Writing Your First Playbook",
	"tags": [],
	"description": "",
	"content": "While Ansible ad hoc commands are useful for simple operations, they are not suited for complex configuration management or orchestration scenarios. For such use cases Playbooks are the way to go.\nPlaybooks are files which describe the desired configurations or steps to implement on managed hosts. Playbooks can change lengthy, complex administrative tasks into easily repeatable routines with predictable and successful outcomes.\nA Playbook is where you can take some of those ad hoc commands you just ran and put them into a repeatable set of Plays and Tasks.\nA Playbook can have multiple Plays and a Play can have one or multiple Tasks. In a Task a Module is called, like the Modules in the previous chapter.\n The goal of a Play is to map a group of hosts to a list of Tasks. The goal of a Task is to implement Modules against those hosts.  Here is a nice analogy: When Ansible Modules are the tools in your workshop, the Inventory is the list of materials and the Playbooks are the instructions.\n What are Collections and why should I care? Ansible Collections are a new distribution format for Ansible content that can include playbooks, roles, modules, and plugins. Ansible Collection names are a combination of two components. The first part is the name of the author who wrote and maintains the Ansible Collection. The second part is the name of the Ansible Collection. This allows one author to have multiple Collections. It also allows multiple authors to have Ansible Collections with the same name.\n\u0026lt;author\u0026gt;.\u0026lt;collection\u0026gt;  These are examples for Ansible Collection names:\n  ansible.posix\n  geerlingguy.k8s\n  theforeman.foreman\n  To identify a specific module in an Ansible Collection, we add the name of it as the third part:\n\u0026lt;author\u0026gt;.\u0026lt;collection\u0026gt;.\u0026lt;module\u0026gt;  Valid examples for a fully qualified Ansible Collection Name (FQCN):\n  ansible.posix.selinux\n  geerlingguy.k8s.kubernetes\n  geerlingguy.php_roles.mysql\n  theforeman.foreman.user\n  Many modules and plugins are part of the \u0026ldquo;ansible.builtin\u0026rdquo; collection and are shipped with Ansible and installed automatically. Although not mandatory, it is highly recommended to also use the FQCN for builtin modules, to avoid name clashes or unpredictable behavior. This is why all following examples, will use \u0026ldquo;ansible.builtin\u0026rdquo; as part of the FQCN.\n If you want to learn more about Ansible Collection, feel free to have a look at our Ansible Collections Workshop\n Playbook Basics Playbooks are text files written in YAML format and therefore need:\n  to start with three dashes (---)\n  proper indentation using spaces and not tabs!\n  There are some important concepts:\n  hosts: the managed hosts to perform the tasks on\n  tasks: the operations to be performed by invoking Ansible modules and passing them the necessary options.\n  become: privilege escalation in Playbooks, same as using -b in the ad hoc command.\n  The ordering of the contents within a Playbook is important, because Ansible executes plays and tasks in the order they are presented.\n A Playbook should be idempotent, so if a Playbook is run once to put the hosts in the correct state, it should be safe to run it a second time and it should make no further changes to the hosts.\nMost Ansible modules are idempotent, so it is relatively easy to ensure this is true.\n Creating a Directory Structure and File for your Playbook Enough theory, it’s time to create your first Playbook. In this lab you create a Playbook to set up an Apache webserver in three steps:\n  First step: Install httpd package\n  Second step: Enable/start httpd service\n  Third step: Create an index.html file\n  This Playbook makes sure the package containing the Apache webserver is installed on node1.\nThere is a best practice on the preferred directory structures for playbooks. We strongly encourage you to read and understand these practices as you develop your Ansible ninja skills. That said, our playbook today is very basic and creating a complex structure will just confuse things.\nInstead, we are going to create a very simple directory structure for our playbook, and add just a couple of files to it.\nIn your browser, bring up your VS Code terminal (you opened one in the first section) and create a directory called ansible-files in your home directory and change into it:\n[ec2-user@autoctl1 ~]$ mkdir ansible-files [ec2-user@autoctl1 ~]$ cd ansible-files/ Now use VS Code to add a file called apache.yml with the following content.\nIf you are unsure how to use VS Code (basically like VSCode), have a quick look at the Visual Studio Code Server introduction\n --- - name: Apache server installed hosts: node1 become: yes This shows one of Ansible’s strengths: The Playbook syntax is easy to read and understand. In this Playbook:\n  A name is given for the play via name:.\n  The host to run the playbook against is defined via hosts:.\n  We enable user privilege escalation with become:.\n  You obviously need to use privilege escalation to install a package or run any other task that requires root permissions. This is done in the Playbook by become: yes.\n Now that we\u0026rsquo;ve defined the play, let\u0026rsquo;s add a task to get something done. We will add a task in which yum will ensure that the Apache package is installed in the latest version. Modify the file so that it looks like the following listing using the VS Code editor:\n--- - name: Apache server installed hosts: node1 become: yes tasks: - name: latest Apache version installed ansible.builtin.yum: name: httpd state: latest  Since playbooks are written in YAML, alignment of the lines and keywords is crucial. Make sure to vertically align the t in task with the b in become. Once you are more familiar with Ansible, make sure to take some time and study a bit the YAML Syntax.\n In the added lines:\n  We started the tasks part with the keyword tasks:.\n  A task is named and the module for the task is referenced. Here it uses the yum module.\n  Parameters for the module are added:\n  name: to identify the package name\n  state: to define the wanted state of the package\n    The module parameters are individual to each module. If in doubt, look them up again with ansible-doc.\n Save your playbook.\nRunning the Playbook Playbooks are executed using the ansible-playbook command on the control node. Before you run a new Playbook it’s a good idea to check for syntax errors. Head over to the VS Code terminal and run:\n[ec2-user@autoctl1 ansible-files]$ ansible-playbook --syntax-check apache.yml Now you should be ready to run your Playbook:\n[ec2-user@autoctl1 ansible-files]$ ansible-playbook apache.yml The output should not report any errors but provide an overview of the tasks executed and a Play Recap summarizing what has been done. There is also a task called Gathering Facts listed: this is a built-in task that runs automatically at the beginning of each Play. It collects information about the managed nodes. Exercises later on will cover this in more detail.\nUse SSH to make sure Apache has been installed on node1. The necessary IP address is provided in the inventory. Grep for the IP address there and use it to SSH to the node.\n[ec2-user@autoctl1 ansible-files]$ grep node1 ~/lab_inventory/hosts node1 ansible_host=11.22.33.44 [ec2-user@autoctl1 ansible-files]$ ssh 11.22.33.44 Last login: Wed May 15 14:03:45 2019 from 44.55.66.77 Managed by Ansible [...] [ec2-user@node1 ~]$ rpm -qi httpd Name : httpd Version : 2.4.6 [...] Log out of node1 with the command exit so that you are back on the control host, and verify the installed package with an Ansible ad hoc command!\n[ec2-user@autoctl1 ansible-files]$ ansible node1 -m command -a \u0026#39;rpm -qi httpd\u0026#39; Run the Playbook a second time, and compare the output: The output changed from changed to ok, and the color changed from yellow to green. Also the PLAY RECAP output is different now. This make it easy to spot what Ansible actually did.\nExtend your Playbook: Start \u0026amp; Enable Apache The next part of the Playbook makes sure the Apache webserver is enabled and started on node1.\nOn the control host, as your student user, edit the file ~/ansible-files/apache.yml again to add a second task using the service module. The Playbook should now look like this:\n--- - name: Apache server installed hosts: node1 become: yes tasks: - name: latest Apache version installed ansible.builtin.yum: name: httpd state: latest - name: Apache enabled and running ansible.builtin.service: name: httpd enabled: true state: started Again: what these lines do is easy to understand:\n  a second task is created and named\n  a module is specified (service)\n  parameters for the module are supplied\n  With the second task we make sure the Apache server is indeed running on the target machine. Run your extended Playbook:\n[ec2-user@autoctl1 ansible-files]$ ansible-playbook apache.yml Note the output now: Some tasks are shown as ok in green and one is shown as changed in yellow.\n  Use an Ansible ad hoc command again to make sure Apache has been enabled and started, e.g. with: systemctl status httpd.\n  Run the Playbook a second time to get used to the change in the output.\n  Extend your Playbook: Create an index.html Check that the tasks were executed correctly and Apache is accepting connections: Make an HTTP request using Ansible’s uri module in an ad hoc command from the control node. Make sure to replace the IP with the IP for the node from the inventory.\nExpect a lot of red lines and a 403 status\n [ec2-user@autoctl1 ansible-files]$ ansible localhost -m uri -a \u0026#34;url=http://\u0026lt;IP\u0026gt;\u0026#34; There are a lot of red lines and an error: As long as there is not at least an index.html file to be served by Apache, it will throw an ugly HTTP Error 403: Forbidden status and Ansible will report an error.\nSo why not use Ansible to deploy a simple index.html file? Create the file ~/ansible-files/index.html on the control node:\n\u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Apache is running fine\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; You already used Ansible’s copy module to write text supplied on the command line into a file. Now you’ll use the module in your Playbook to actually copy a file:\nOn the control node as your student user edit the file ~/ansible-files/apache.yml and add a new task utilizing the copy module. It should now look like this:\n--- - name: Apache server installed hosts: node1 become: yes tasks: - name: latest Apache version installed ansible.builtin.yum: name: httpd state: latest - name: Apache enabled and running ansible.builtin.service: name: httpd enabled: true state: started - name: copy index.html ansible.builtin.copy: src: ~/ansible-files/index.html dest: /var/www/html/ You are getting used to the Playbook syntax, so what happens? The new task uses the copy module and defines the source and destination options for the copy operation as parameters.\nRun your extended Playbook:\n[ec2-user@autoctl1 ansible-files]$ ansible-playbook apache.yml   Have a good look at the output\n  Run the ad hoc command using the \u0026ldquo;uri\u0026rdquo; module from further above again to test Apache: The command should now return a friendly green \u0026ldquo;status: 200\u0026rdquo; line, amongst other information.\n  Practice: Apply to Multiple Host This was nice but the real power of Ansible is to apply the same set of tasks reliably to many hosts.\n So what about changing the apache.yml Playbook to run on node1 and node2 and node3?  As you might remember, the inventory lists all nodes as members of the group web:\n[web] node1 ansible_host=11.22.33.44 node2 ansible_host=22.33.44.55 node3 ansible_host=33.44.55.66  The IP addresses shown here are just examples, your nodes will have different IP addresses.\n Change the Playbook to point to the group web:\n--- - name: Apache server installed hosts: web become: yes tasks: - name: latest Apache version installed ansible.builtin.yum: name: httpd state: latest - name: Apache enabled and running ansible.builtin.service: name: httpd enabled: true state: started - name: copy index.html ansible.builtin.copy: src: ~/ansible-files/index.html dest: /var/www/html/ Now run the Playbook:\n[ec2-user@autoctl1 ansible-files]$ ansible-playbook apache.yml Finally check if Apache is now running on all servers. Identify the IP addresses of the nodes in your inventory first, and afterwards use them each in the ad hoc command with the uri module as we already did with the node1 above. All output should be green.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/",
	"title": "Ansible Collections",
	"tags": [],
	"description": "",
	"content": "Ansible Collections Workshop  Exercise 1 - Introduction Exercise 2 - Collections from playbook Exercise 3 - Collections from roles Exercise 4 - Collections from tower Exercise 5 - Creating Collections Exercise 6 - Use Automation Hub  Additional Resources  Ansible Docs: Using Collections Ansible Collections Overview Ansible Docs: Developing Collections Blog Post: Introducing: The AWX and Ansible Tower Collections  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/4-using-collections-from-tower/",
	"title": "Collections in automation controller",
	"tags": [],
	"description": "",
	"content": "Red Hat Ansible Tower supports Ansible Collections starting with version 3.5 - earlier version will not automatically install and configure them for you. To make sure Ansible Collections are recognized by Red Hat Ansible Tower a requirements file is needed and has to be stored in the proper directory.\nAnsible Galaxy is already configured by default, however if you want your Red Hat Ansible Tower to prefer and fetch content from the Red Hat Automation Hub, additional configuration changes are required. They are addressed in a the chapter Use Automation Hub in this lab.\nIn this exercise you will learn how to define an Ansible Collection as a requirement in a format recognized by Red Hat Ansible Tower. We\u0026rsquo;ll be using the ansible.posix collection again from Ansible Galaxy. As you probably know for Ansible Tower to access the needed bits and pieces a version control system is needed. For the sake of keeping this lab setup easy, we\u0026rsquo;ll set up a local Git server for this.\nSet up a Local Git Server So, let’s get started. We have to create a simplistic Git server on our control host. Typically you would work with GitHub, GitLab, Gitea, or any other Git server.\nMake sure to run these steps from your users home directory!\n [ec2-user@autoctl1 ~]$ wget https://raw.githubusercontent.com/goetzrieger/ansible-labs-summit2020/master/content/ansible-collections/4-using-collections-from-tower/simple_git.yml [ec2-user@autoctl1 ~]$ ansible-playbook simple_git.yml -e \u0026quot;git_project=tower_collections\u0026quot; Next we will clone the repository on the control host. To enable you to work with git on the command line the SSH key for user ec2-user was already added to the Git user git. Next, clone the repository on the control machine:\n[ec2-user@autoctl1 ~]$ git clone git@ansible-1:projects/tower_collections # Message \u0026#34;warning: You appear to have cloned an empty repository.\u0026#34; is OK and can be ignored [ec2-user@autoctl1 ~]$ git config --global push.default simple [ec2-user@autoctl1 ~]$ git config --global user.name \u0026#34;Your Name\u0026#34; [ec2-user@autoctl1 ~]$ git config --global user.email you@example.com [ec2-user@autoctl1 ~]$ cd tower_collections/  The repository is currently empty. The three config commands are just there to avoid warnings from Git.\n You now have a local Git server that can be accessed via SSH from Tower.\nCreate Content for the Ansible Tower Project Red Hat Ansible Tower can download and install Ansible Collections automatically before executing a Job Template. If a collections/requirements.yml exists in your project, it will be parsed and the Ansible Collections specified in this file will be automatically installed.\nStarting with Red Hat Ansible Tower 3.6 the working directory for the Job Template is in /tmp. Since the Ansible Collection is downloaded into this directory before the Job Template is executed, you will not find temporary files of your Ansible Collection in /var/lib/awx/projects/.\n The format of the requirements.yml for Ansible Collections is very similar to the one for roles, however it is very important to store in the folder collections.\nLet\u0026rsquo;s create the files needed to see how you can use collections in Ansible Tower. This is of course just a simple example. First create the collections directory in your Git repo (you should have changed into tower_collections already above):\n[ec2-user@autoctl1 tower_collections]$ mkdir collections [ec2-user@autoctl1 tower_collections]$ cd collections Then create the requirements.yml file listing the collection(s) you need:\n--- collections: - ansible.posix As this is a simple example we\u0026rsquo;ll just add a Playbook to the Git repo now. Normally you would have a lot more content in your project repository. So what could we do as an example instead of using the ansible.posix collection again? Let\u0026rsquo;s create a Playbook to configure an at job:\n--- - name: Install AT Job hosts: all tasks: - name: at command needs to be installed yum: name: at state: present - name: Schedule a command to execute in 20 minutes as root ansible.posix.at: command: ls -d / \u0026gt;/dev/null count: 20 units: minutes Save the file as configure_at_job.yml.\nNote the usage of the Fully Qualified Collection Name in the Playbook\n Make sure everything looks fine:\n[ec2-user@autoctl1 tower_collections]$ tree . ├── collections │ └── requirements.yml └── configure_at_job.yml So far you created the code only locally on the control host, now you are ready to add it to the repository and push it:\n[ec2-user@autoctl1 tower_collections]$ git add collections configure_at_job.yml [ec2-user@autoctl1 tower_collections]$ git commit -m \u0026#34;Adding requirements.yml and playbook\u0026#34; [ec2-user@autoctl1 tower_collections]$ git push Create the Project and Job Template Now it\u0026rsquo;s time to access your Ansible Tower web UI if you haven\u0026rsquo;t done so out of curiosity already. Point your browser to the URL you were given on the lab landing page, similar to https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com (replace \u0026lt;N\u0026gt; with your student number and \u0026lt;LABID\u0026gt; with the ID of this lab) and log in as admin. You can find the password again on the lab landing page.\nTo run your new Playbook in Ansible Tower you have to configure a number of objects:\n An Inventory with the managed hosts Machine Credentials to access the managed hosts Git Credentials to access your Git repository via SSH The Git repo as Project A Job Template to run the Playbook  We\u0026rsquo;ll be a bit verbose for students new to Ansible Tower. If you are an Ansible Tower old-hand, just skip through and finish the configuration steps shown.\nInventory and Machine Credentials The inventory Workshop Inventory and the machine credentials Workshop Credentials have already been created in your lab environment.\nConfigure SCM Credentials Now we will configure the credentials to access our the Git repo on your control host via SSH. In the RESOURCES menu choose Credentials. Now:\nClick the button to add new credentials\n NAME: Git Credentials ORGANIZATION: Click on the magnifying glass, pick Default and click SELECT CREDENTIAL TYPE: Click on the magnifying glass, pick Source Control as type and click SELECT (you will have to use the search or cycle through the types to find it). USERNAME: ec2-user  As we are using SSH key authentication, you have to provide an SSH private key that can be used to access the host with the Git repo as the user git.\nThe Playbook we used to configure Git added the SSH private key to the authorized_keys of user git\n Bring up your VS Code server terminal on Tower and use cat to get the SSH private key:\n[bastion.\u0026lt;GUID\u0026gt;.internal ~]$ cat ~/.ssh/aws-private.pem -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEA2nnL3m5sKvoSy37OZ8DQCTjTIPVmCJt/M02KgDt53+baYAFu1TIkC3Yk+HK1 [...] -----END RSA PRIVATE KEY-----   Copy the complete private key (including \u0026ldquo;BEGIN\u0026rdquo; and \u0026ldquo;END\u0026rdquo; lines) from the output and paste it into the SSH PRIVATE KEY field in the web UI.\n  Click SAVE\n  You have now setup credentials to access the Git repo on your control host.\nSet up the Project It\u0026rsquo;s time to set up the Tower Project pointing to your Git repository holding your Playbook and collections requirements file.\n Go to RESOURCES → Projects in the side menu view click the button. Fill in the form: NAME: Collections Repo ORGANIZATION: Default SCM TYPE: Git  Now you need the URL to access the repo. You could get the URL in Github as Clone URL. Enter the URL into the Project configuration:\n SCM URL: git@bastion.\u0026lt;GUID\u0026gt;.internal:projects/tower_collections SCM CREDENTIAL: Click the magnifying glass and choose Git Credentials, click SELECT SCM UPDATE OPTIONS: Tick the first three boxes to always get a fresh copy of the repository and to update the repository when launching a job. Click SAVE  The new Project will be synced automatically after creation. If everything went fine, you should see a green icon to the left of the new Project.\nCreate the Job Template and run it The last step is to create a Job Template to run the Playbook. Go to the Templates view, click the button and choose Job Template.\n  NAME: Install AT Job\n  JOB TYPE: Run\n  INVENTORY: Workshop Inventory\n  PROJECT: Collections Repo\n  PLAYBOOK: configure_at_job.yml\n  CREDENTIAL: Workshop Credentials\n  We need to run the tasks as root so check Enable privilege escalation\n  Click SAVE\n  You can start the job by directly clicking the blue LAUNCH button, or by clicking on the rocket in the Job Templates overview. After launching the Job Template, you are automatically brought to the job overview where you can follow the playbook execution in real time.\nAfter the Job has finished bring up your VSCode terminal, as the job was run on all three managed nodes and the control/ Ansible Tower node, you can simply check the result here. Run sudo at -l and you should see your job was scheduled successfully.\nTroubleshooting Since Red Hat Ansible Tower does only check for updates in the repository in which you stored your Playbook, it might not do a refresh if there was a change in the Ansible Collection used by your Playbook. This happens particularly if you also combine Roles and Collections.\nIn this case you should check the option Delete on Update which will delete the entire local directory during a refresh.\nIf there is a problem while parsing your requirements.yml it\u0026rsquo;s worth testing it with the ansible-galaxy command. As a reminder, Red Hat Ansible Tower basically also just runs the command for you with the appropriate parameters, so testing this works manually makes a lot of sense.\nansible-galaxy collection install -r collections/requirements.yml -f  The -f switch will forces a fresh installation of the specified Ansible Collections, otherwise ansible-galaxy will only install it, if it wasn\u0026rsquo;t already installed. You can also use the --force-with-deps switch to make sure Ansible Collections which have dependencies to others are refreshed as well.\n "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/4-cluster-jobs/",
	"title": "Run a Job in a Cluster",
	"tags": [],
	"description": "",
	"content": "After boot-strapping the controller configuration from bottom up you are ready to start a job in your controller cluster. In one of your controller nodes web UI’s:\n  Open the Templates view\n  Look for the Install Apache Template you created with the Playbook\n  Run it by clicking the rocket icon.\n  At first this is not different from a standard controller setup. But as this is a cluster of active controller instances each instance could have run the job.\nSo, which Instance did actually run the Job? There are a couple of ways to find the node that executed the job.\nFrom the Job Output The most obvious way is to look up the Execution Node in the details of the job output. If you closed it already or want to look it up later, go to Views-\u0026gt;Jobs and look up the job in question.\nFrom the instance groups In one of the controller instances web UI under Administration go to the Instance Groups view. For the controlplane instance group, the Total Jobs counter shows the number of finished jobs. If you click the controlplane instance group and switch to the Jobs tab in the next page, you’ll get a detailed list of jobs.\nTo see how jobs are distributed over the instances in an Instance Groups, go to the Instances tab under the controlplane group, you will get an overview of the Total Jobs each controller instance in this group executed.\nUsing the API   First find the job ID: In the web UI access Views→Jobs\n  The jobs names are prefixed with the job ID, example 3 - Install Apache\n  Make sure you choose a job with type \u0026ldquo;Playbook run\u0026rdquo;.\n  With the ID you can query the API for the instance/node the job was executed on  Bring up the terminal in your VSCode session and run:\nReplace \u0026lt;ID\u0026gt; with the job ID you want to query and VERY_SECRET_PASSWORD with your actual password. And don\u0026rsquo;t forget the proper \u0026lt;GUiD\u0026gt;. Just saying\u0026hellip;\n [lab-user@bastion ~]$ curl -s -k -u admin:VERY_SECRET_PASSWORD https://autoctl1.\u0026lt;GUID\u0026gt;.internal/api/v2/jobs/\u0026lt;ID\u0026gt;/ | python3 -m json.tool | grep execution_node \u0026#34;execution_node\u0026#34;: \u0026#34;autoctl1.\u0026lt;GUID\u0026gt;.internal\u0026#34;,  You can use any method you want to access the API and to display the result, of course. The usage of curl and python was just an example.\n Via API in the browser Another way to query the controller API is using a browser. For example to have a look at the job details (basically what you did above using curl and friends):\nNote you used the internal hostname above, when using your browser, you have to use the external hostname, of course.\n   Find the job ID\n  Now get the job details via the API interface:\n  Login to the API with user admin and password VERY_SECRET_PASSWORD: https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com/api/\n  Open the URL https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com/api/v2/jobs/\u0026lt;ID\u0026gt;/ where \u0026lt;ID\u0026gt; is the number of the job you just looked up in the UI.\n  Search the page for the string you are interested in, e.g. execution_node\n    You can of course query any controller node.\n "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/4-surveys/",
	"title": "Surveys",
	"tags": [],
	"description": "",
	"content": "You might have noticed the Survey tab when looking at a Template. A survey is a way to create a simple form to ask for parameters that get used as variables when a Template is launched as a Job.\nYou have installed Apache on all hosts in the job you just run. Now we’re going to extend on this:\n  Use a proper role which has a Jinja2 template to deploy an index.html file.\n  Create a job Template with a survey to collect the values for the index.html template.\n  Launch the job Template.\n  Additionally, the role will also make sure that the Apache configuration is properly set up - in case it got mixed up during the other exercises.\nThe survey feature only provides a simple query for data - it does not support four-eye principles, queries based on dynamic data or nested menus.\n The Apache-configuration Role The Playbook and the role with the Jinja template already exist in the directory rhel/apache in the Github repository https://github.com/ansible-labs-crew/playbooks_summit2021 you already configured as a Project.\nHead over to the Github UI and have a look at the content: the playbook apache_role_install.yml merely references the role. The role can be found in the roles/role_apache subdirectory.\n  Inside the role, note the two variables in the templates/index.html.j2 template file marked by {{…​}}.\n  Also, check out the tasks in tasks/main.yml that deploy the file from the template.\n  What is this Playbook doing? It creates a file (dest) on the managed hosts from the template (src).\nThe role also deploys a static configuration for Apache. This is to make sure that all changes done in the previous chapters are overwritten and your examples work properly.\nBecause the Playbook and role is located in the same Github repo as the apache_install.yml Playbook you don\u0026rsquo;t have to configure a new project for this exercise.\nCreate a Template with a Survey Now you create a new Template that includes a survey.\nCreate Template   Go to Templates, click the button and choose Add job template\n  Name: Create index.html\n  Configure the template to use:\n  Webserver as Inventory.\n  Ansible Workshop Examples as the Project.\n  Default execution environment for the Execution Environment.\n  apache_role_install.yml as the Playbook to execute.\n  Workshop Credentials as credentials.\n  privilege escalation.\n    Try for yourself, the solution is below.\nClick here for Solution     Name: Create index.html\n  Job Type: Run\n  Inventory: Webserver\n  Project: Ansible Workshop Examples\n  Execution Environment: Default execution environment\n  Playbook: rhel/apache/apache_role_install.yml\n  Credentials: Workshop Credentials\n  Options: Privilege Escalation\n  Click Save\n    Do not run the template yet!\n Add the Survey   In the Template, go to the Survey tab\n  Click Add and fill in:\n  Question: First Line\n  Answer variable name: first_line\n  Answer Type: Text\n    Click Save\n  In the same way add a second survey question\n  Question: Second Line\n  Answer variable name: second_line\n  Answer type: Text\n    Click Save\n  Now click the blue Preview button to see how the survey is going to look. Close the preview again.\nTo enable the survey, while still on the Survey tab switch the slider button to On\nLaunch the Template Now launch the Create index.html job template.\nBefore the actual launch the survey will ask for First Line and Second Line. Fill in some text and click Next. The next window shows the launch details and at the bottom the values the survey has prompted for. If all is good run the Job by clicking Launch.\nNote how the two survey lines are shown on the Details tab of the Job view as Variables.\n After the job has completed, check the Apache homepage. In your VS Code terminal, execute curl against http://node1.\u0026lt;GUID\u0026gt;.internal:\n[lab-user@bastion ~]$ curl http://node1.\u0026lt;GUID\u0026gt;.internal \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Apache is running fine\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;This is survey field \u0026#34;First Line\u0026#34;: line one\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;This is survey field \u0026#34;Second Line\u0026#34;: line two\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; Note how the two variables where used by the playbook to create the content of the index.html file.\nWhat About Some Practice? Here is a list of tasks:\nPlease make sure to finish these steps as the next chapter depends on it!\n   Take the inventory Webserver and add node node2.\u0026lt;GUID\u0026gt;.internal to it.\n  Run the Create index.html Template again.\n  Verify the results on http://node2.\u0026lt;GUID\u0026gt;.internal by using curl.\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/4-variables/",
	"title": "Using Variables",
	"tags": [],
	"description": "",
	"content": "Previous exercises showed you the basics of Ansible Engine. In the next few exercises, we are going to teach some more advanced Ansible skills that will add flexibility and power to your playbooks.\nAnsible exists to make tasks simple and repeatable. We also know that not all systems are exactly alike and often require some slight change to the way an Ansible playbook is run. Enter variables.\nAnsible supports variables to store values that can be used in Playbooks. Variables can be defined in a variety of places and have a clear precedence. Ansible substitutes the variable with its value when a task is executed.\nVariables are referenced in Playbooks by placing the variable name in double curly braces:\nHere comes a variable {{ variable1 }} Variables and their values can be defined in various places: the inventory, additional files, on the command line, etc.\nThe recommended practice to provide variables in the inventory is to define them in files located in two directories named host_vars and group_vars:\n  To define variables for a group servers, a YAML file named group_vars/servers with the variable definitions is created.\n  To define variables specifically for a host node1, the file host_vars/node1 with the variable definitions is created.\n  Host variables take precedence over group variables (more about precedence can be found in the docs).\n Create Variable Files For understanding and practice let’s do a lab. Following up on the theme \u0026ldquo;Let’s build a webserver. Or two. Or even more…​\u0026rdquo;, you will change the index.html to show the development environment (dev/prod) a server is deployed in.\nOn the ansible control host, as the student user, create the directories to hold the variable definitions in ~/ansible-files/:\n[ec2-user@autoctl1 ansible-files]$ mkdir host_vars group_vars Now create two files containing variable definitions. We’ll define a variable named stage which will point to different environments, dev or prod:\n Create the file ~/ansible-files/group_vars/web with this content:  --- stage: dev  Create the file ~/ansible-files/host_vars/node2 with this content:  --- stage: prod What is this about?\n  For all servers in the web group the variable stage with value dev is defined. So as default we flag them as members of the dev environment.\n  For server node2 this is overridden and the host is flagged as a production server.\n  Create index.html Files Now create two files in ~/ansible-files/:\nOne called prod_index.html with the following content:\n\u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a production webserver, take care!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; And the other called dev_index.html with the following content:\n\u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a development webserver, have fun!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; Create the Playbook Now you need a Playbook that copies the prod or dev index.html file - according to the \u0026ldquo;stage\u0026rdquo; variable.\nCreate a new Playbook called deploy_index_html.yml in the ~/ansible-files/ directory.\nNote how the variable \u0026ldquo;stage\u0026rdquo; is used in the name of the file to copy.\n --- - name: Copy index.html hosts: web become: yes tasks: - name: copy index.html ansible.builtin.copy: src: ~/ansible-files/{{ stage }}_index.html dest: /var/www/html/index.html  Run the Playbook:  [ec2-user@autoctl1 ansible-files]$ ansible-playbook deploy_index_html.yml Test the Result The Playbook should copy different files as index.html to the hosts, use curl to test it. Check the inventory again if you forgot the IP addresses of your nodes.\n[ec2-user@autoctl1 ansible-files]$ grep node ~/lab_inventory/hosts node1 ansible_host=11.22.33.44 node2 ansible_host=22.33.44.55 node3 ansible_host=33.44.55.66 [ec2-user@autoctl1 ansible-files]$ curl http://11.22.33.44 \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a development webserver, have fun!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; [ec2-user@autoctl1 ansible-files]$ curl http://22.33.44.55 \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a production webserver, take care!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; [ec2-user@autoctl1 ansible-files]$ curl http://33.44.55.66 \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a development webserver, have fun!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt;  If by now you think: There has to be a smarter way to change content in files…​ you are absolutely right. This lab was done to introduce variables, you are about to learn about templates in one of the next chapters.\n Ansible Facts Ansible facts are variables that are automatically discovered by Ansible from a managed host. Remember the \u0026ldquo;Gathering Facts\u0026rdquo; task listed in the output of each ansible-playbook execution? At that moment the facts are gathered for each managed node. Facts can also be pulled by the setup module. They contain useful information stored into variables that administrators can reuse.\nTo get an idea what facts Ansible collects by default, on your control node as your student user run:\n[ec2-user@autoctl1 ansible-files]$ ansible node1 -m setup This might be a bit too much, you can use filters to limit the output to certain facts, the expression is shell-style wildcard:\n[ec2-user@autoctl1 ansible-files]$ ansible node1 -m setup -a \u0026#39;filter=ansible_eth0\u0026#39; Or what about only looking for memory related facts:\n[ec2-user@autoctl1 ansible-files]$ ansible node1 -m setup -a \u0026#39;filter=ansible_*_mb\u0026#39; Challenge Lab: Facts  Try to find and print the distribution (Red Hat) of your managed hosts. On one line, please.  Use grep to find the fact, then apply a filter to only print this fact.\n Click here for Solution   [ec2-user@autoctl1 ansible-files]$ ansible node1 -m setup|grep distribution [ec2-user@autoctl1 ansible-files]$ ansible node1 -m setup -a \u0026#39;filter=ansible_distribution\u0026#39; -o \n  Using Facts in Playbooks Facts can be used in a Playbook like variables, using the proper naming, of course. Create this Playbook as facts.yml in the ~/ansible-files/ directory:\n--- - name: Output facts within a playbook hosts: all tasks: - name: Prints Ansible facts ansible.builtin.debug: msg: The default IPv4 address of {{ ansible_fqdn }} is {{ ansible_default_ipv4.address }}  The \u0026ldquo;debug\u0026rdquo; module is handy for e.g. debugging variables or expressions.\n Execute it to see how the facts are printed:\n[ec2-user@autoctl1 ansible-files]$ ansible-playbook facts.yml PLAY [Output facts within a playbook] ****************************************** TASK [Gathering Facts] ********************************************************* ok: [node3] ok: [node2] ok: [node1] ok: [ansible] TASK [Prints Ansible facts] **************************************************** ok: [node1] =\u0026gt; msg: The default IPv4 address of node1 is 172.16.190.143 ok: [node2] =\u0026gt; msg: The default IPv4 address of node2 is 172.16.30.170 ok: [node3] =\u0026gt; msg: The default IPv4 address of node3 is 172.16.140.196 ok: [ansible] =\u0026gt; msg: The default IPv4 address of ansible is 172.16.2.10 PLAY RECAP ********************************************************************* ansible : ok=2 changed=0 unreachable=0 failed=0 node1 : ok=2 changed=0 unreachable=0 failed=0 node2 : ok=2 changed=0 unreachable=0 failed=0 node3 : ok=2 changed=0 unreachable=0 failed=0 "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/5-instance-groups/",
	"title": "automation controller instance groups",
	"tags": [],
	"description": "",
	"content": "Automation controller clustering allows you to easily add capacity to your controller infrastructure by adding instances. In a single-group controller cluster where all instances are within the controlplane group there is no way to influence which node will run a job, the cluster will take care of scheduling Jobs as it sees fit.\nTo enable more control over which node is running a job, Tower 3.2 saw the introduction of the Instance Groups feature. Instance groups allow you to organize your cluster nodes into groups. In turn Jobs can be assigned to Instance Groups by configuring the Groups in Organizations, Inventories or Job Templates.\nThe order of priority is Job Template \u0026gt; Inventory \u0026gt; Organization. So Instance Groups configured in Job Templates take precedence over those configured in Inventories, which take precedence over Organizations\n Some things to keep in mind about Instance Groups:\n  Nodes in an Instance Group share a job queue.\n  You can have as many Instance Groups as you like as long as there is at least one node in the controlplane group.\n  Nodes can be in one or more Instance Groups.\n  Groups can not be named instance_group_controlplane!\n  Controller instances can’t have the same name as a group.\n  Instance Groups allows some pretty cool setups, e.g. you could have some nodes shared over the whole cluster (by putting them into all groups) but then have other nodes that are dedicated to one group to reserve some capacity.\nThe base controlplane group does house keeping like processing events from jobs for all groups so the node count of this group has to scale with your overall cluster load, even if these nodes are not used to run Jobs.\n Talking about the controlplane group: As you have learned this group is crucial for the operations of a controller cluster. Apart from the house keeping tasks, if a resource is not associated with an Instance Group, one of the nodes from the controlplane group will run the Job. So if there are no operational nodes in the base group, the cluster will not be able to run Jobs.\nIt is important to have enough nodes in the controlplane group\n There is a great blog post going into Instance Groups with a lot more depth.\n Instance Group Setup Having the introduction out of the way, let’s get back to our lab and give Instance Groups a try.\nIn a basic cluster setup like ours you just have the controlplane base group. So let’s go and setup two instance groups:\n  In Instance Groups add a new group by clicking the icon and then Add instance group\n  Name the new group dev\n  Save\n  Go to the Instances tab of the new Instances Group and add node autoctl2.\u0026lt;GUID\u0026gt;.internal by clicking the Associate button.\n  Do the same to create a the new group prod with instance autoctl3.\u0026lt;GUID\u0026gt;.internal\nGo back to the Instance Groups view, you should now have the following setup:\n  All instances are in the controlplane base group\n  Two more groups (prod and dev) with one instances each\n  We’re using the internal names of the controller nodes here.\n This is not best practice, it’s just for the sake of this lab! Any jobs that are launched targeting a group without active nodes will be stuck in a waiting state until instances become available. So one-instance groups are never a good idea.\n Verify Instance Groups You can check your instance groups in a number of ways.\nVia the Web UI You have configured the groups here, open the URL\nhttps://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com/#/instance_groups  in your browser.\nIn the INSTANCE GROUPS overview all instance groups are listed with details of the group itself like number of instances in the group, running jobs and total jobs. Like you’ve seen before the current capacity of the instance groups is shown in a live view, thus providing a quick insight if there are capacity problems.\nVia the API You can again query the API to get this information. Either use the browser to access the URL (you might have to login to the API again):\nhttps://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com/api/v2/instance_groups/\nor use curl to access the API from the command line in your VSCode terminal:\n[lab-user@bastion ~]$ curl -s -k -u admin:VERY_SECRET_PASSWORD https://autoctl1.\u0026lt;GUID\u0026gt;.internal/api/v2/instance_groups/| python3 -m json.tool\nThe curl command has to be on one line. Do not forget or oversee the final slash at the end of the URL, it is relevant!\n Deactivating automation controller instances While in the Instances Groups overview in the web UI click the dev group. In the next view change to the Instances tab, at the right you’ll see a slide button for each controller instance (only one in this case).\n  The button should be set to On meaning \u0026ldquo;active\u0026rdquo;. Clicking it would deactivate the corresponding instance and would prevent that further jobs are assigned to it.\n  Running jobs on an instance which is set to Off are finished in a normal way.\n  The slider Capacity Adjustment can change the amount of forks scheduled on an instance. This way it is possible to influence in which ratio the jobs are assigned.\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/5-handlers/",
	"title": "Conditionals, Handlers and Loops",
	"tags": [],
	"description": "",
	"content": "Conditionals Ansible can use conditionals to execute tasks or plays when certain conditions are met.\nTo implement a conditional, the when statement must be used, followed by the condition to test. The condition is expressed using one of the available operators like e.g. for comparison:\n         == Compares two objects for equality.   != Compares two objects for inequality.   \u0026gt; true if the left hand side is greater than the right hand side.   \u0026gt;= true if the left hand side is greater or equal to the right hand side.   \u0026lt; true if the left hand side is lower than the right hand side.   \u0026lt;= true if the left hand side is lower or equal to the right hand side.    There are many options to control execution flow in Ansible. More examples of supported conditionals can be located here: http://jinja.pocoo.org/docs/dev/templates/#comparisons\nAs an example you would like to install an FTP server, but only on hosts that are in the ftpserver inventory group.\nTo do that, first edit the inventory to add another group, and place node2 in it. Make sure that the IP address of node2 is always the same when node2 is listed. Edit the inventory ~/lab_inventory/hosts to look like the following listing:\n[all:vars] ansible_user=student\u0026lt;GUID\u0026gt; ansible_ssh_pass=xxx ansible_port=22 [web] node1 ansible_host=11.22.33.44 node2 ansible_host=22.33.44.55 node3 ansible_host=33.44.55.66 [ftpserver] node2 ansible_host=22.33.44.55 [control] ansible ansible_host=44.55.66.77 Next create the file ftpserver.yml on your control host in the ~/ansible-files/ directory:\n--- - name: Install vsftpd on ftpservers hosts: all become: yes tasks: - name: Install FTP server when host in ftpserver group ansible.builtin.yum: name: vsftpd state: latest when: inventory_hostname in groups[\u0026quot;ftpserver\u0026quot;]  By now you should know how to run Ansible Playbooks, we’ll start to be less verbose in this guide. Go create and run it. :-)\n Run it and examine the output. The expected outcome: The task is skipped on node1, node3 and the ansible host (your control host) because they are not in the ftpserver group in your inventory file.\nTASK [Install FTP server when host in ftpserver group] ******************************************* skipping: [ansible] skipping: [node1] skipping: [node3] changed: [node2] Handlers Sometimes when a task does make a change to the system, an additional task or tasks may need to be run. For example, a change to a service’s configuration file may then require that the service be restarted so that the changed configuration takes effect.\nHere Ansible’s handlers come into play. Handlers can be seen as inactive tasks that only get triggered when explicitly invoked using the notify statement. Read more about them in the Ansible Handlers documentation.\nAs a an example, let’s write a Playbook that:\n  manages Apache’s configuration file httpd.conf on all hosts in the web group\n  restarts Apache when the file has changed\n  First we need the file Ansible will deploy, let’s just take the one from node1. Remember to replace the IP address shown in the listing below with the IP address from your individual node1.\n[ec2-user@autoctl1 ansible-files]$ scp 11.22.33.44:/etc/httpd/conf/httpd.conf ~/ansible-files/ httpd.conf Next, create the Playbook httpd_conf.yml. Make sure that you are in the directory ~/ansible-files.\n--- - name: manage httpd.conf hosts: web become: yes tasks: - name: Copy Apache configuration file ansible.builtin.copy: src: httpd.conf dest: /etc/httpd/conf/ notify: - restart_apache handlers: - name: restart_apache ansible.builtin.service: name: httpd state: restarted So what’s new here?\n  The notify section calls the handler only when the copy task actually changes the file. That way the service is only restarted if needed - and not each time the playbook is run.\n  The handlers section defines a task that is only run on notification.\n  Run the Playbook. We didn’t change anything in the file yet so there should not be any changed lines in the output and of course the handler shouldn’t have fired.\n Now change the Listen 80 line in httpd.conf to:  Listen 8080   Run the Playbook again. Now Ansible’s output should be a lot more interesting:\n  httpd.conf should have been copied over\n  The handler should have restarted Apache\n    Apache should now listen on port 8080. Easy enough to verify:\n[ec2-user@autoctl1 ansible-files]$ curl http://22.33.44.55 curl: (7) Failed connect to 22.33.44.55:80; Connection refused [ec2-user@autoctl1 ansible-files]$ curl http://22.33.44.55:8080 \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a production webserver, take care!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; Feel free to change the httpd.conf file again and run the Playbook.\nSimple Loops Loops enable us to repeat the same task over and over again. For example, lets say you want to create multiple users. By using an Ansible loop, you can do that in a single task. Loops can also iterate over more than just basic lists. For example, if you have a list of users with their corresponding group, loop can iterate over them as well. Find out more about loops in the Ansible Loops documentation.\nTo show the loops feature we will generate three new users on node1. For that, create the Playbook loop_users.yml in ~/ansible-files on your control node as your student user and run it. We will use the user module to generate the user accounts.\n--- - name: Ensure users hosts: node1 become: yes tasks: - name: Ensure three users are present ansible.builtin.user: name: \u0026quot;{{ item }}\u0026quot; state: present loop: - dev_user - qa_user - prod_user  Lines starting with a variable need to be quoted.\n Understand the playbook and the output:\n  The names are not provided to the user module directly. Instead, there is only a variable called {{ item }} for the parameter name.\n  The loop keyword lists the actual user names. Those replace the {{ item }} during the actual execution of the playbook.\n  During execution the task is only listed once, but there are three changes listed underneath it.\n  Loops over hashes As mentioned loops can also be over lists of hashes. Imagine that the users should be assigned to different additional groups:\n- username: dev_user groups: ftp - username: qa_user groups: ftp - username: prod_user groups: apache The user module has the optional parameter groups to list additional users. To reference items in a hash, the {{ item }} keyword needs to reference the subkey: {{ item.groups }} for example.\nLet\u0026rsquo;s rewrite the playbook to create the users with additional user rights:\n--- - name: Ensure users hosts: node1 become: yes tasks: - name: Ensure three users are present ansible.builtin.user: name: \u0026quot;{{ item.username }}\u0026quot; state: present groups: \u0026quot;{{ item.groups }}\u0026quot; loop: - { username: 'dev_user', groups: 'ftp' } - { username: 'qa_user', groups: 'ftp' } - { username: 'prod_user', groups: 'apache' } Check the output:\n Again the task is listed once, but three changes are listed. Each loop with its content is shown.  Verify that the user prod_user was indeed created on node1:\n[ec2-user@autoctl1 ansible-files]$ ansible node1 -m command -a \u0026#34;id dev_user\u0026#34; node1 | CHANGED | rc=0 \u0026gt;\u0026gt; uid=1002(dev_user) gid=1002(dev_user) groups=1002(dev_user),50(ftp) "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/5-creating-collections/",
	"title": "Creating Collections",
	"tags": [],
	"description": "",
	"content": "In this exercise you will learn how to create your own custom collection.\nPreparing the exercise environment To keep the content of this exercise separated from the other exercises you\u0026rsquo;ve done, first create a directory in your lab named exercise-05 and cd into it. This directory will be used during the whole exercise. In your VSCode terminal, run the following commands:\n[ec2-user@autoctl1 ~]$ mkdir exercise-05 [ec2-user@autoctl1 ~]$ cd exercise-05 [ec2-user@autoctl1 exercise-05]$ This was covered already in this lab, but it\u0026rsquo;s important enough to state again: Ansible Collections have two default lookup paths that are searched.\n  User scoped path /home/\u0026lt;username\u0026gt;/.ansible/collections\n  System scoped path /usr/share/ansible/collections\n  Users can customize the collections path by modifying the collections_path key in the ansible.cfg file or by setting the environment variable ANSIBLE_COLLECTIONS_PATHS with the desired search path.\n Inspecting the structure of a collection Ansible Collections have a standard directory and file structure that can hold modules, plugins, roles and playbooks.\ncollection/ ├── docs/ ├── galaxy.yml ├── plugins/ │ ├── modules/ │ │ └── module1.py │ ├── inventory/ │ └── .../ ├── README.md ├── roles/ │ ├── role1/ │ ├── role2/ │ └── .../ ├── playbooks/ └── tests/ Here is a short description of the collection structure:\n  The plugins folder holds plugins, modules, and module_utils that can be reused in playbooks and roles.\n  The roles folder hosts custom roles, while all collection playbooks must be stored in the playbooks folder.\n  The docs folder can be used for the collections documentation, as well as the main README.md file that is used to describe the collection and its content.\n  The tests folder holds tests written for the collection.\n  The galaxy.yml file is a YAML text file that contains all the metadata used in the Ansible Galaxy hub to index the collection. It is also used to list collection dependencies, if there are any.\n  Creating a Collection Okay, with the introduction out of the way, let\u0026rsquo;s create a custom collection and populate it with roles, playbook, plugins and modules. A scaffold for a user defined custom collection can be created manually or with the ansible-galaxy collection init command.\nCreate the Structure Let\u0026rsquo;s get started, in your VSCode terminal create the initial structure for your new collection:\n[ec2-user@autoctl1 exercise-05]$ ansible-galaxy collection init --init-path ansible_collections redhat.workshop_demo_collection - Collection redhat.workshop_demo_collection was created successfully The --init-path flag is used to define a custom path in which the skeleton will be initialized. The collection name always follows the pattern \u0026lt;namespace\u0026gt;.\u0026lt;collection\u0026gt;. The above example creates the workshop_demo_collection in the redhat namespace.\nHave a look for yourself:\n[ec2-user@autoctl1 exercise-05]$ tree . └── ansible_collections └── redhat └── workshop_demo_collection ├── docs ├── galaxy.yml ├── plugins │ └── README.md ├── README.md └── roles You can see the namespace directory was created together with a top-level ansible_collections directory. The scaffold is pretty minimal, note the template README files and a template galaxy.yml file is created to define Galaxy metadata.\nAdding Content: Custom Modules and Plugins Now it\u0026rsquo;s time to add content to our collection scaffold. Collections can include different kinds of plugins and modules. For a complete list of types please refer to the README.md file in the plugins folder.\nIn this lab we are going to create a minimal Hello World module and install it in the plugins/modules directory.\nFirst, create the plugins/modules directory:\n[ec2-user@autoctl1 exercise-05]$ cd ansible_collections/redhat/workshop_demo_collection [ec2-user@autoctl1 workshop_demo_collection}} ]$ mkdir plugins/modules Using the VSCode editor, create the file demo_hello.py with the content below in the modules folder. The demo_hello module says, well, \u0026ldquo;Hello\u0026rdquo; in different languages to a user defined through a parameter. This is not a lab about writing plugins, but take the time to look at the module code and understand its behavior.\nWhen doing copy/paste from your browser into the VSCode editor you might have to use Shift-Ctrl-V.\n #!/usr/bin/python  ANSIBLE_METADATA = { \u0026#39;metadata_version\u0026#39;: \u0026#39;1.0\u0026#39;, \u0026#39;status\u0026#39;: [\u0026#39;preview\u0026#39;], \u0026#39;supported_by\u0026#39;: \u0026#39;community\u0026#39; } DOCUMENTATION = \u0026#39;\u0026#39;\u0026#39; --- module: demo_hello short_description: A module that says hello in many languages version_added: \u0026#34;2.8\u0026#34; description: - \u0026#34;A module that says hello in many languages.\u0026#34; options: name: description: - Name of the person to salute. If no value is provided the default value will be used. required: false type: str default: John Doe author: - Gianni Salinetti (@giannisalinetti) \u0026#39;\u0026#39;\u0026#39; EXAMPLES = \u0026#39;\u0026#39;\u0026#39; # Pass in a custom name - name: Say hello to Linus Torvalds demo_hello: name: \u0026#34;Linus Torvalds\u0026#34; \u0026#39;\u0026#39;\u0026#39; RETURN = \u0026#39;\u0026#39;\u0026#39; fact: description: Hello string type: str sample: Hello John Doe! \u0026#39;\u0026#39;\u0026#39; import random from ansible.module_utils.basic import AnsibleModule FACTS = [ \u0026#34;Hello {name}!\u0026#34;, \u0026#34;Bonjour {name}!\u0026#34;, \u0026#34;Hola {name}!\u0026#34;, \u0026#34;Ciao {name}!\u0026#34;, \u0026#34;Hallo {name}!\u0026#34;, \u0026#34;Hei {name}!\u0026#34;, ] def run_module(): module_args = dict( name=dict(type=\u0026#39;str\u0026#39;, default=\u0026#39;John Doe\u0026#39;), ) module = AnsibleModule( argument_spec=module_args, supports_check_mode=True ) result = dict( changed=False, fact=\u0026#39;\u0026#39; ) result[\u0026#39;fact\u0026#39;] = random.choice(FACTS).format( name=module.params[\u0026#39;name\u0026#39;] ) if module.check_mode: return result module.exit_json(**result) def main(): run_module() if __name__ == \u0026#39;__main__\u0026#39;: main() An Ansible module is basically an implementation of the AnsibleModule class created and executed in a minimal function called run_module(). As you can see, a module has a main() function, like a plain Python executable. Anyway, it is not meant to be executed independently.\nAdding Content: Adding a Custom Role Ansible Collections are about bundling content that belongs together. So in the last step of this exercise we\u0026rsquo;ll create a role inside the custom collection that utilizes the new module. To make things easy it will simply write the greeting into the Message of the Day file /etc/motd.\nGenerate the new role hello_motd using the ansible-galaxy init command:\nThe ansible-galaxy command can be used to create initial directory structures for collections and roles. Make sure you are in the root directory of your ansible collection before executing this command.\n [ec2-user@autoctl1 workshop_demo_collection ]$ ansible-galaxy init --init-path roles hello_motd - Role hello_motd was created successfully In the next step add the following role tasks in the roles/hello_motd/tasks/main.yml file:\n--- # tasks file for hello_motd - name: Generate greeting and store result demo_hello: name: \u0026quot;{{ friend_name }}\u0026quot; register: demo_greeting - name: store test in /etc/motd copy: content: \u0026quot;{{ demo_greeting.fact }}\\n\u0026quot; dest: /etc/motd become: yes Notice the usage of the demo_hello module, installed in the collection, to generate the greeting string.\nWhen a collection role calls a module in the same collection namespace, the module is automatically resolved.\n Every role should come with sensible defaults, add the following default variable to the roles/hello_motd/defaults/main.yml file to make it look like this:\n--- # defaults file for hello_motd friend_name: \u0026quot;John Doe\u0026quot; Because ansible-galaxy creates a complete structure of directories and files, it\u0026rsquo;s a good idea to clean up unused ones to keep it tidy:\n[ec2-user@autoctl1 workshop_demo_collection ]$ rm -rf roles/hello_motd/{handlers,vars,tests} And as the final step customize the roles/hello_motd/meta/main.yml file to define Galaxy metadata and potential dependencies of the role. Use this sample minimal content:\ngalaxy_info: author: Ansible Workshop Team description: Hello world demo license: GPL-2.0-or-later min_ansible_version: 2.9 galaxy_tags: [\u0026quot;demo\u0026quot;] dependencies: [] Build and Install your Custom Collection Okay, you are done with creating your role. Now you\u0026rsquo;ll build the collection and generate a .tar.gz file that can be installed locally or uploaded to Galaxy. From the collection folder run the following command:\n[ec2-user@autoctl1 workshop_demo_collection ]$ ansible-galaxy collection build Created collection for redhat.workshop_demo_collection at /home/student\u0026lt;GUID\u0026gt;/exercise-05/ansible_collections/redhat/workshop_demo_collection/redhat-workshop_demo_collection-1.0.0.tar.gz The above command will create the file redhat-workshop_demo_collection-1.0.0.tar.gz. Notice the semantic x.y.z versioning. Once created the file can be installed in the COLLECTIONS_PATH to be tested locally:\n[ec2-user@autoctl1 workshop_demo_collection ]$ ansible-galaxy collection install redhat-workshop_demo_collection-1.0.0.tar.gz Process install dependency map Starting collection install process Installing \u0026#39;redhat.workshop_demo_collection:1.0.0\u0026#39; to \u0026#39;/home/student\u0026lt;GUID\u0026gt;/.ansible/collections/ansible_collections/redhat/workshop_demo_collection\u0026#39; By default the collection will be installed in the ~/.ansible/collections/ansible_collections folder. Now the collection can be used locally!\nTesting your Collection Create the exercise-05/collections_test folder to hold the local test:\n[ec2-user@autoctl1 workshop_demo_collection ]$ cd ~/exercise-05 [ec2-user@autoctl1 exercise-05 ]$ mkdir collections_test [ec2-user@autoctl1 exercise-05 ]$ cd collections_test To test the collection you need a basic playbook.yml file, create it with the following content:\n--- - hosts: localhost tasks: - import_role: name: redhat.workshop_demo_collection.hello_motd vars: friend_name: \u0026quot;Angry Potato\u0026quot; Running the Test Playbook Run the test playbook.\n[ec2-user@autoctl1 collections_test ]$ ansible-playbook playbook.yml PLAY [localhost] ****************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************ ok: [localhost] TASK [redhat.workshop_demo_collection.hello_motd : Generate greeting and store result] ******************************** ok: [localhost] TASK [redhat.workshop_demo_collection.hello_motd : store test in /etc/motd] ******************************************* changed: [localhost] PLAY RECAP ************************************************************************************************************ localhost : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Verify the result by viewing the content of /etc/motd:\n[ec2-user@autoctl1 collections_test ]$ cat /etc/motd Hello Angry Potato!  The Module is creating the text in a random language, so your greeting might differ from the example above.\n Takeaways   Collections can be created using the ansible-galaxy collection init command. Users can develop collections contents accordingly to their needs and business logic.\n  Collections plugins can be either any kind of Ansible plugins or modules. Modules are developed inside collection to create an autonomous lifecycle from the main Ansible upstream.\n  Collection roles can use local collections plugins and modules.\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/5-rbac/",
	"title": "Role-based access control",
	"tags": [],
	"description": "",
	"content": "You have already learned how automation controller separates credentials from users. Another advantage of automation controller is the user and group rights management.\nAutomation controller Users There are three types of automation controller users:\n  Normal User: Have read and write access limited to the inventory and projects for which that user has been granted the appropriate roles and privileges.\n  System Auditor: Auditors implicitly inherit the read-only capability for all objects within the automation controller environment.\n  System Administrator: Has admin, read, and write privileges over the entire automation controller installation.\n  Let’s create a user:\n  In the automation controller web UI menu under Access choose Users\n  Click the button\n  Fill in the values for the new user:\n  Username: wweb\n  Email: wweb@example.com\n  Password: ansible\n  Confirm password\n  First Name: Werner\n  Last Name: Web\n  User Type: Normal User\n    Click Save\n  Automation controller Teams A Team is a subdivision of an organization with associated users, projects, credentials, and permissions. Teams provide a means to implement role-based access control schemes and delegate responsibilities across organizations. For instance, permissions may be granted to a whole Team rather than each user on the Team.\nCreate a Team:\n  Go to Access → Teams.\n  Click the button and create a team named Web Content.\n  Now you can add a user to the Team:\n  Return to Access -\u0026gt; Users and click the wweb user.\n  Jump to the Teams tab of user wweb.\n  Click the Associate button and check the Web Content Team.\n  Click Save\n  User wweb is now a member of the Web Content Team.\n  Granting Permissions Permissions allow to read, modify, and administer projects, inventories, and other automation controller elements. Permissions can be set for different resources.\nTo allow users or teams to actually do something, you have to set permissions. The members of the Team Web Content should only be allowed to modify content of the assigned webservers.\nAdd the permission to use the template:\n  Open the Team Web Content.\n  Go to the Roles tab and click the button.\n  A new window opens. You can choose to set permissions for a number of resources.\n  Select the resource type Job Templates\n  Click Next\n  Choose the Create index.html Template by checking the box next to it.\n  Click Next\n  Choose the role Execute\n    Click Save\n  Test Permissions Now log out of automation controller’s web UI and in again as the wweb user.\n  Go to the Templates view, you should notice for wweb only the Create index.html template is listed. He is allowed to view and launch, but not to edit the Template. Just open the template and try to change it, there is not even an Edit button.\n  Run the Job Template by clicking the rocket icon. Enter the survey content to your liking and launch the job.\n  In the following Jobs view have a good look around, note that there where changes to the host (of course…​).\n  Check the result: In the VS Code terminal execute curl to pull the content of the webserver on node1.\u0026lt;GUID\u0026gt;.internal (you could of course check node2.\u0026lt;GUID\u0026gt;.internal, too):\n[lab-user@bastion ~]$ curl http://node1.\u0026lt;GUID\u0026gt;.internal   In the web UI, log out user wweb and in again as admin.  Just recall what you have just done: You enabled a restricted user to run an Ansible Playbook\n  Without having access to the credentials.\n  Without being able to change the Playbook itself.\n  But with the ability to change variables you predefined!\n  Effectively you provided the power to execute automation to another user without handing out your credentials or giving the user the ability to change the automation code. And yet, at the same time the user can still modify things based on the surveys you created.\nThis capability is one of the main strengths of automation controller!\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/6-parallel-jobs/",
	"title": "Parallel Jobs",
	"tags": [],
	"description": "",
	"content": "The real power of instance groups is revealed when multiple jobs are started, and they are assigned to different controller nodes. To launch parallel jobs we will set up a workflow with multiple concurrent jobs.\nLab Scenario To configure something meaningful we\u0026rsquo;ll make a quick detour into security automation here. During this lab we’ll focus on security compliance according to STIG, CIS and so on. Often these compliance rules are enforced by executing an Ansible task per each requirement. This makes documentation and audit easier.\nCompliance requirements are often grouped into independent categories. The tasks can often be executed in parallel because they do not conflict with each other.\nIn our demo case we use three playbooks which:\n  ensure the absence of a few packages (STIG)\n  ensure configuration of PAM and login cryptography (STIG)\n  ensure absence of services and kernel modules (CIS).\n  The Playbooks can be found in the Github repository you already setup as a Project in your controller.\nPrepare the Compliance Lab Create three Templates As mentioned the Github repository you configured as a Project contains three Playbooks to enforce different compliance requirements. Since you learned in the previous chapter how to configure automation controller with the AWX Collection from a Playbook, we want to put that knowledge to the test. Create a new Playbook in your VScode editor on the bastion node that creates the needed job Templates for the provided Ansible Playbooks in automation controller. Create one task for each of the three Ansible Playbooks:\n  stig-packages.yml (task name Compliance STIG packages)\n  stig-config.yml (task name Compliance STIG config)\n  cis.yml (task name Compliance CIS)\n  You can basically use the task definition AWX Job Template from the Playbook configure-controller.yml and adapt the task\u0026rsquo;s name and playbook parameters. With one important addition:\n Change the Execution Environment by adding execution_environment: Ansible Engine 2.9 execution environment to each task. This is needed because the default Execution Environment doesn\u0026rsquo;t contain a module we use in the Playbooks (modprobe)  If you closed your VSCode terminal or lost connection, the environment variables with the connection parameters are gone. Just set them again by sourcing the set-connection.sh file you created before.\n Click here for Solution   --- - name: Configure automation controller hosts: localhost become: false gather_facts: false tasks: - name: Compliance STIG packages Job Template awx.awx.job_template: name: Compliance STIG packages organization: Default state: present inventory: AWX inventory become_enabled: True playbook: stig-packages.yml project: AWX Project credential: AWX Credentials execution_environment: Ansible Engine 2.9 execution environment - name: Compliance STIG config Job Template awx.awx.job_template: name: Compliance STIG config organization: Default state: present inventory: AWX inventory become_enabled: True playbook: stig-config.yml project: AWX Project credential: AWX Credentials execution_environment: Ansible Engine 2.9 execution environment - name: Compliance CIS Job Template awx.awx.job_template: name: Compliance CIS organization: Default state: present inventory: AWX inventory become_enabled: True playbook: cis.yml project: AWX Project credential: AWX Credentials execution_environment: Ansible Engine 2.9 execution environment \n  Create Parallel Workflow To enable parallel execution of the tasks in these job templates, we will create a workflow using the web UI. Workflows are configured in the Templates view, you might have noticed you can choose between Add job template and Add workflow template when adding a template.\n  Go to the Templates view and click the button. Choose Add workflow template\n  Name: Compliance Workflow\n  Organization: Default - click on the magnifying glass if necessary\n  Click Save\n    Now the Workflow Visualizer graphical workflow designer opens.\n  Click on the Start button, a dialog to configure a new workflow node opens.\n  First select the node type, you can choose between Job Template, Project Sync, Inventory Source Sync, Approval and Workflow Job Template.\n  Select Job Template\n  In this lab we’ll link multiple jobs to the Start, so select the Compliance STIG packages job template your Playbook configured and click Save. The new workflow node gets annotated with the name of the job.\n  Hover your mouse over the now blue Start button and click the + sign, the Add Node dialog opens again.\n  This time select the Compliance STIG config job template and hit Save.\n  Hover your mouse over the Start button again, and configure the last job template.\n  Select the Compliance CIS job template and click Save.\n  Click the blue Save button at the upper right to save the workflow.\n  Your workflow is now ready, you can always open the workflow visualizer again by clicking the Visualizer tab of the workflow.\n  You have configured a Workflow that is not going through templates one after the other but rather executes three templates in parallel.\nExecute and Watch Your workflow is ready to go, launch it.\n  In the Templates view launch the Compliance Workflow by clicking the rocket icon.\n  Wait until the workflow has finished.\n  Go to the Instance Groups view and find out how the jobs where distributed over the instances:\n  Open the controlplane instance group by clicking it and switch to the Instances tab.\n  Look at the Total Jobs view of the three instances\n  Because the Job Templates called in the workflow didn’t specify an instance group, they where distributed (more or less) evenly over the instances.\n  Deactivate a node Now deactivate instance autoctl1.\u0026lt;GUID\u0026gt;.internal by setting the slider button to Off and wait until it is shown as unavailable. Make a (mental) note of the Total Jobs counter of the instance. Go back to the list of templates and launch the workflow Compliance Workflow again.\nGo back to the Instance Groups view, get back to the instance overview of instance group controlplane and verify that the three Playbooks where launched on the remaining instances and the Total Jobs counter of instance autoctl1.\u0026lt;GUID\u0026gt;.internal didn’t change.\nActivate autoctl1.\u0026lt;GUID\u0026gt;.internal by sliding the button to On again.\nUsing Instance Groups So we have seen how a automation controller cluster is distributing jobs over instances by default. We have already created instance groups which allow us to take control over which job is executed on which node, so let’s use them.\nTo make it easier to spot where the jobs were run, let’s first empty the jobs history. This can be done using the awx-manage utility on one of the automation controller instances. From your VSCode terminal as lab-user SSH to your autoctl1.\u0026lt;GUID\u0026gt;.internal node, become user awx and run the actual command:\n[lab-user@bastion ~]$ ssh autoctl1.\u0026lt;GUID\u0026gt;.internal [ec2-user@autoctl1 ~]$ sudo -u awx -i [awx@autoctl1 ~]$ awx-manage cleanup_jobs --days=0 Then exit back to being lab-user on the bastion node again. If you now check the Total Jobs of your instances they should show count \u0026ldquo;0\u0026rdquo;.\nAssign Jobs to Instance Groups One way to assign a job to an instance group is in the job template. As our compliance workflow uses three job templates, do this for all of them:\n  In the web UI, go to Templates\n  Open one of the three (Compliance CIS, Compliance STIG config, Compliance STIG packages) compliance templates (not the workflow template)\n  Click Edit\n  In the Instance Group field, choose the dev instance group and click Select.\n  Click Save for the job template!\n  Do this for the other two compliance templates, too.\n  Now the jobs that make up our Compliance Workflow are all configured to run on the instances of the dev instance group.\nRun the Workflow You have done this a couple of times now, you should get along without detailed instructions.\n  Run the Compliance Workflow\n  What would you expect? On what instance(s) should the workflow jobs run?\n  Verify!\n  Result: The workflow and the associated jobs will run on autoctl2.\u0026lt;GUID\u0026gt;.internal. Okay, big surprise, in the dev instance group there is only one instance.\n But what’s going to happen if you disable this instance?\n  Disable the autoctl2.\u0026lt;GUID\u0026gt;.internal instance on the Instances tab of Instance Group dev by setting the switch to off.\n  Run the workflow again.\n  What would you expect? On what instance(s) should the workflow jobs run?\n  Verify!\n  Result: The workflow is running but the associated jobs will stay in pending state because there are no instances available in the `dev\u0026rsquo; instance group, and the workflow runs \u0026ldquo;forever\u0026rdquo;.\n What’s going to happen if you enable the instance again?\n  Go to the Instances tab of the instance group dev again and enable autoctl2.\u0026lt;GUID\u0026gt;.internal.\n  Check in the Jobs and Instance Groups view what’s happening.\n  Result: After the instance is enabled again the jobs will pickup and run on autoctl2.\u0026lt;GUID\u0026gt;.internal.\n At this point make sure the instances you disabled in the previous steps are definitely enabled again! Otherwise subsequent lab tasks might fail…\n "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/6-templates/",
	"title": "Templates",
	"tags": [],
	"description": "",
	"content": "Ansible uses Jinja2 templating to modify files before they are distributed to managed hosts. Jinja2 is one of the most used templating engines for Python.\nUsing Templates in Playbooks When a template for a file has been created, it can be deployed to the managed hosts using the template module, which supports the transfer of a local file from the control node to the managed hosts.\nAs an example of using templates you will change the motd file to contain host-specific data.\nFirst in the ~/ansible-files/ directory create the template file motd-facts.j2:\nWelcome to {{ ansible_hostname }}. {{ ansible_distribution }} {{ ansible_distribution_version}} deployed on {{ ansible_architecture }} architecture. The template file contains the basic text that will later be copied over. It also contains variables which will be replaced on the target machines individually.\nNext we need a playbook to use this template. In the ~/ansible-files/ directory create the Playbook motd-facts.yml:\n--- - name: Fill motd file with host data hosts: node1 become: yes tasks: - name: create MOTD file ansible.builtin.template: src: motd-facts.j2 dest: /etc/motd owner: root group: root mode: 0644 You have done this a couple of times by now:\n  Understand what the Playbook does.\n  Execute the Playbook motd-facts.yml.\n  Login to node1 via SSH and check the message of the day content.\n  Log out of node1.\n  You should see how Ansible replaces the variables with the facts it discovered from the system.\nChallenge Lab Add a line to the template to list the current kernel of the managed node.\n Find a fact that contains the kernel version using the commands you learned in the \u0026ldquo;Ansible Facts\u0026rdquo; chapter.  Do a grep -i for kernel\n   Change the template to use the fact you found.\n  Run the Playbook again.\n  Check motd by logging in to node1\n   Click here for Solution     Find the fact:  [ec2-user@autoctl1 ansible-files]$ ansible node1 -m setup|grep -i kernel \u0026#34;ansible_kernel\u0026#34;: \u0026#34;3.10.0-693.el7.x86_64\u0026#34;,  Modify the template motd-facts.j2:  Welcome to {{ ansible_hostname }}. {{ ansible_distribution }} {{ ansible_distribution_version}} deployed on {{ ansible_architecture }} architecture running kernel {{ ansible_kernel }}.  Run the playbook. Verify the new message via SSH login to node1.    "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/6-automation-hub-and-galaxy/",
	"title": "Use Automation Hub",
	"tags": [],
	"description": "",
	"content": "Automation Hub and Ansible Galaxy Red Hat Automation Hub Automation Hub is a service that is provided as part of the Red Hat SaaS offering to subscribers of Ansible Automation Platform. It is a central location where supported and certified Ansible Content Collections by Red Hat and its Partners can be found, downloaded and integrated into your Ansible automation. The support for Automation Hub is included with Red Hat Automation Platform subscription.\nRed Hat Automation Hub resides on https://cloud.redhat.com/ansible/automation-hub and requires Red Hat customer portal credentials and a valid and active Red Hat Automation Platform subscription.\n Certified Content In the portal of Automation Hub, users have direct access to certified content collections from Red Hat and Partners. Certified collections are developed, tested, built, delivered, and supported by Red Hat and its Partners. To find more details about the scope of support, check the Ansible Certified Content FAQ,\nSupported Automation Automation Hub is a one-stop-shop for Ansible content that is backed by support from Red Hat to deliver additional reassurance for customers. Additional supportability claims for these collections may be provided under the \u0026ldquo;Maintained and Supported By\u0026rdquo; one of Red Hat Partners. A list of currently supported content can be found in the Knowledge base.\nAnsible Galaxy Ansible Galaxy is the upstream location for the Ansible community that initially started to provide pre-packaged units of work known as Ansible roles. Roles can be used from Ansible Playbooks and immediately put to work. in a recent version of Galaxy started to provide Ansible content collections as well.\nAnsible Galaxy resides on https://galaxy.ansible.com/\nAccessing collections from Automation Hub Ansible collections can be used and downloaded from multiple locations. They can either be downloaded using a requirement file, statically included in the git repository or eventually installed separately in the virtual environment.\nThis is not an exercise you can actually run in this environment because you would need to have an account to Ansible Automation Hub that comes with a subscription of Ansible Automation Platform. It is here for your information.\n Authenticate Ansible Tower to Automation Hub Creating a token Authenticating Ansible Tower requires a token. It can be achieved using the steps below:\n  Navigate to https://cloud.redhat.com/ansible/automation-hub/token/\n  Click Load Token.\n  Click copy icon to copy the API token to the clipboard.\n  Using authentication token   As user admin, navigate to the Settings l\u0026gt; Jobs\n  Set PRIMARY GALAXY SERVER URL to https://cloud.redhat.com/api/automation-hub/\n  Set PRIMARY GALAXY AUTHENTICATION URL to https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token\n  Set PRIMARY GALAXY SERVER TOKEN to \u0026lt;COPIED_TOKEN\u0026gt;\n  It is recommended using Red Hat Automation Hub as primary Galaxy Server URL to ensure using certified and supported content by Red Hat and its partners via Red Hat Ansible Automation subscription.\n Using collections After authenticating Ansible Tower to access Automation Hub, using a collections/requirements.yml file automatically fetches the content collections from Automation Hub as first source.\nTakeaways  Ansible Galaxy hosts upstream community content. The Red Hat Automation Hub provides certified collections that are supported by Red Hat and its Partners. It\u0026rsquo;s a service provided by the Red Hat Ansible Automation Platform Subscription. Red Hat Ansible Tower can be configured to authenticate to Red Hat Automation Hub in order to fetch certified and supported content collections that are utilized in a given project.  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/6-workflows/",
	"title": "Workflows",
	"tags": [],
	"description": "",
	"content": "The basic idea of an automation controller workflow is to link multiple Job Templates together. They may or may not share inventory, Playbooks or even permissions. The links can be conditional:\n  If job template A succeeds, job template B is automatically executed afterwards.\n  In case of a failure, job template C will be run.\n  And the workflows are not even limited to Job Templates, but can also include project or inventory updates.\nThis enables new applications for automation controller: different Job Templates can build upon each other. E.g. the networking team creates playbooks with their own content, in their own Git repository and even targeting their own inventory, while the operations team also has their own repos, playbooks and inventory.\nIn this lab you’ll learn how to setup a workflow.\nLab Scenario You have two departments in your organization:\n  The web operations team that is developing Playbooks for deploying web infrastructures in their own Git repository.\n  The web applications team, that develops JavaScript web applications for NodeJS in their Git repository.\n  When there is a new NodeJS-based website to deploy, two main steps need to happen:\nThe web operations team has to:\n  Install and configure NodeJS to run as a service.\n  An Apache instance needs to be installed and configured as proxy to pass requests for the NodeJS content to the NodeJS backend. And a lot of other steps might be needed, too. Like SELinux, firewall\u0026hellip; you know the drill.\n  The web developer team has to:\n  Deploy the most recent version of the JavaScript web application.\n  Make sure everything is prepared to run the application like the directory structure and service restarts.\n  To make things somewhat easier for you, everything needed already exists in a Github repository: Playbooks, JavaScript files etc. You just need to glue it together.\nSet up Projects First you have to set up the Git repo as Projects like you normally would. You have done this before, try to do this on your own. Detailed instructions can be found below.\nIf you are still logged in as user wweb, log out of and log in as user admin again.\n   Create the project for web operations:\n  It should be named Webops Git Repo.\n  The URL to access the repo is https://github.com/ansible-labs-crew/playbooks_ops_summit2021.git\n    Create the project for the application developers:\n  It should be named Webdev Git Repo.\n  The URL to access the repo is https://github.com/ansible-labs-crew/playbooks_dev_summit2021.git\n    Click here for Solution     Create the project for web operations. In the Projects view click the blue Add button and fill in:\n  Name: Webops Git Repo\n  Organization: Default\n  Source Control Credential Type: Git\n  Source Control URL: https://github.com/ansible-labs-crew/playbooks_ops_summit2021.git\n  Options: Clean, Delete, Update Revision on Launch\n    Click Save\n  Create the project for the application developers. In the Projects view click the blue Add button and fill in:\n  Name: Webdev Git Repo\n  Organization: Default\n  Source Control Credential Type: Git\n  Source Control URL: https://github.com/ansible-labs-crew/playbooks_dev_summit2021.git\n  Options: Clean, Delete, Update Revision on Launch\n    Click Save\n    Set up Job Templates Now you have to create Job Templates like you would for \u0026ldquo;normal\u0026rdquo; Jobs.\nWe want to install the NodeJS app on node3 only. The Inventory Workshop Inventory contains all nodes, but we can limit the nodes using the LIMIT field!\n   Go to the Templates view, click the and choose Add job template:\n  Name: Web Infra Deploy\n  Job Type: Run\n  Inventory: Workshop Inventory\n  Project: Webops Git Repo\n  Execution Environment: Default execution environment\n  Playbook: web_infrastructure.yml\n  Credentials: Workshop Credentials\n  Limit: node3.\u0026lt;GUID\u0026gt;.internal\n  Options: Privilege Escalation\n    Click Save\n  Go to the Templates view again, and choose Add job template:\n  Name: Web App Deploy\n  Job Type: Run\n  Inventory: Workshop Inventory\n  Project: Webdev Git Repo\n  Execution Environment: Default execution environment\n  Playbook: install_node_app.yml\n  Credentials: Workshop Credentials\n  Limit: node3,\u0026lt;GUID\u0026gt;.internal\n  OPTIONS: Privilege Escalation\n    Click Save\n  If you want to know what the Playbooks look like, check out the Github URL.\n Set up the Workflow And now you finally set up the Workflow. Workflows are configured in the Templates view, you might have noticed you can choose between Add job template and Add workflow template when adding a template so this is finally making sense.\n  Go to the Templates view and click the button. This time choose Add workflow template\n  Name: Deploy Webapplication\n  Organization: Default\n    Click Save\n  After saving the template the Workflow Visualizer opens to allow you to build a workflow. You can later open the Workflow Visualizer again by using the button on the template details page.\n  Click on the Start button, a dialog to configure a new workflow node opens.\n  First select the node type, you can choose between Job Template, Project Sync, Inventory Source Sync, Approval and Workflow Job Template.\n  Select Job Template\n  Choose the Web Infra Deploy Template, you might have to switch pages.\n  Click Save.\n  You get into the Workflow Visualizer overview showing the first workflow node.\n  The node gets annotated with the name of the job.\n  Hover the mouse pointer over the node, you’ll see a number of options appearing:\n  + to add a new workflow node\n  i for Job Template details\n  a pencil icon to edit this node\u0026rsquo;s settings\n  a link icon to link to another node\n  and a delete icon.\n    We want to add another workflow node, click the + sign\n  You could now set the condition under which the next node is executed, select On Success\n  Click Next\n  The type allows for more complex workflows. You could lay out different execution paths for successful and for failed Playbook runs.\n   Choose Web App Deploy as the next Job.\n  Click Save\n  Back in the main Workflow Visualizer view click Save in the upper right to save the Workflow Template.\n  The Workflow Visualizer has options for setting up more advanced workflows, please refer to the documentation.\n And Action Your Deploy Webapplication workflow is ready to go, launch it.\n Click the Launch button directly or go to the the Templates view and launch the Deploy Webapplication workflow by clicking the rocket icon.  Note how the workflow run is shown in the job view as a visual representation of the different workflow steps. Same as for a normal job template executions you can go to the Details tab to get more information. If you want to look at the actual Jobs behind the workflow nodes, click the workflow nodes. If you want to get back from a details view to the corresponding workflow, just hit your browsers back button.\nAfter the job has finished, check if everything worked fine. In your VS Code terminal, run:\n[lab-user@bastion ~]$ curl http://node3.\u0026lt;GUID\u0026gt;.internal/nodejs  You should be greeted with a friendly Hello World\nWorkflow Approvals To make Workflows in automation controller even more useful you can add approval steps in your Workflow. Using this feature it\u0026rsquo;s possible to have another user review and approve all or certain nodes in a Workflow. In addition you could set time limits, so a Workflow waiting for approval would be cancelled or proceed after time runs out automatically.\nLet\u0026rsquo;s see how this works by adding an approval step to the Deploy Webapplication Workflow Template you configured above. The goal is to:\n  Allow user wweb to execute the workflow by allowing the team Web Content access.\n  To have a user do the approval.\n  Require an approval before the application is installed.\n  Start with inserting the approval into the workflow:\n  As user admin open the Workflow by going to Resources -\u0026gt; Templates -\u0026gt; Deploy Webapplication.\n  Click Visualizer to open the Workflow editor.\n  Hover the mouse over the green connection line between the to nodes and click + to add another node.\n  As run condition choose Always.\n  Click Next\n  Set Node Type to Approval\n  Name: approve app install\n  Save and Save again for the workflow.\n  Now give members of the team Web Content access rights to execute the Workflow. You did this in the RBAC excercise, basically do this:\n  Go to the Access tab of the Workflow.\n  Click and add team Web Content.\n  Select the role Execute\n  Save\n  Log out and log in again as user wweb with password ansible.\n  You should be able to execute the Workflow Template Deploy Webapplication\n  Launch it\n  The workflow should be stuck in the approve app install step, waiting for approval.\n  In the upper right corner you\u0026rsquo;ll see a notification 1 near to the bell icon. Click it.\n   You\u0026rsquo;ll get to the Workflow Approvals view listing pending approvals. If you check the box to the right of the approval, you\u0026rsquo;ll note you can\u0026rsquo;t approve because the Approve and Deny buttons stay inactive.  No surprise, you don\u0026rsquo;t have the permissions as user wweb. Log in as admin again and create the approval user (you did this before in the RBAC exercise):\n  Username: aapproval\n  Password: ansible\n  First Name: Anton\n  Last name: Approval\n  User Type: Normal User\n  Save\n  You could of course approve the workflow as user admin, but we\u0026rsquo;d like to create a dedicated user for this!\n Now go back to the Workflow Deploy Webapplication\n  Open the Access tab\n  Add a aapproval as new user\n  Give the role Approve\n  Save\n  You\u0026rsquo;re set. The Workflow run wweb started is still in pending state, you can check this by going to Views -\u0026gt; Jobs, it will still be shown as running. Okay, let\u0026rsquo;s approve it!\n  Log out and in as user aapproval\n  You\u0026rsquo;ll again see the bell notification in the upper right, click it to get to the Workflow Approvals view.\n  Check the box to the left of the approve app install job.\n  You should now see the Approve and Deny buttons getting active!\n  Click the Approve button.\n  Go to Views -\u0026gt; Jobs and click the Deploy Webapplicattion job.\n  You\u0026rsquo;ll see how the job picks up after you approved it and goes to the Web App Deploy node to finish.\nThis was a basic example to show how you could create a Job, allow one user to execute it and another user to approve it!\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/7-advanced-inventories/",
	"title": "Advanced Inventories",
	"tags": [],
	"description": "",
	"content": "In Ansible and automation controller, as you know, everything starts with an inventory. There are a several methods how inventories can be created, starting from simple static definitions over importing inventory files to dynamic and smart inventories.\nIn real life it’s very common to deal with external dynamic inventory sources (think cloud, CMDB, containers, \u0026hellip;). In this chapter we’ll introduce you to building dynamic inventories using custom inventory scripts. Another great feature of the automation controller to deal with inventories is the Smart Inventory feature which you’ll do a lab on as well.\nDynamic Inventories Quite often just using static inventories will not be enough. You might be dealing with ever-changing cloud environments or you have to get your managed systems from a CMDB or other sources of truth.\nController includes built-in support for syncing dynamic inventory from cloud sources such as Amazon AWS, Google Compute Engine, among others. Controller also offers the ability to use custom inventory scripts to pull the data from your own inventory source.\nIn this chapter you’ll get started with dynamic inventories in the automation controller. Aside from the built-in sources you can write inventory scripts in any programming/scripting language that you have installed on the controller machine. To keep it easy we’ll use a most simple custom inventory script using\u0026hellip; Bash! Yes!\nDon’t get this wrong\u0026hellip; we’ve chosen to use Bash to make it as simple as possible to show the concepts behind dynamic and custom inventories. Usually you’d use Python or some other scripting/programming language.\n The Inventory Source First you need a source. In real life this would be your cloud provider, your CMDB or what not. For the sake of this lab we put a simple file into a Github repository.\nUse curl to query your \u0026ldquo;external inventory source\u0026rdquo;:\n[lab-user@bastion ~]$ curl https://raw.githubusercontent.com/ansible-labs-crew/playbooks_adv_summit2021/master/inventory/inventory_list { \u0026#34;dyngroup\u0026#34;:{ \u0026#34;hosts\u0026#34;:[ \u0026#34;cloud1.cloud.example.com\u0026#34;, \u0026#34;cloud2.cloud.example.com\u0026#34; ], \u0026#34;vars\u0026#34;:{ \u0026#34;var1\u0026#34;: true } }, \u0026#34;_meta\u0026#34;:{ \u0026#34;hostvars\u0026#34;:{ \u0026#34;cloud1.cloud.example.com\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;web\u0026#34; }, \u0026#34;cloud2.cloud.example.com\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;database\u0026#34; } } } } Well, this is handy, the output is already presented as JSON, the way Ansible would expect\u0026hellip; ;-)\nOkay, seriously, in real life your script would likely get some information from your source system, format it as JSON and return the data to the automation controller.\n The Custom Inventory Script An inventory script has to follow some conventions. It must accept the \u0026ndash;list and \u0026ndash;host \u0026lt;hostname\u0026gt; arguments. When it is called with \u0026ndash;list, the script must output a JSON-encoded data containing all groups and hosts to be managed. When called with \u0026ndash;host \u0026lt;hostname\u0026gt; it must return an JSON-formatted hash or dictionary of host variables (can be empty).\nAs looping over all hosts and calling the script with \u0026ndash;host can be pretty slow, it is possible to return a top level element called \u0026ldquo;_meta\u0026rdquo; with all of the host variables in one script run. And this is what we’ll do. So this is our custom inventory script:\n#!/bin/bash  if [ \u0026#34;$1\u0026#34; == \u0026#34;--list\u0026#34; ] ; then curl -sS https://raw.githubusercontent.com/ansible-labs-crew/playbooks_adv_summit2021/master/inventory/inventory_list elif [ \u0026#34;$1\u0026#34; == \u0026#34;--host\u0026#34; ]; then echo \u0026#39;{\u0026#34;_meta\u0026#34;: {\u0026#34;hostvars\u0026#34;: {}}}\u0026#39; else echo \u0026#34;{ }\u0026#34; fi What it basically does is to return the data collected by curl when called with \u0026ndash;list and as the data includes _meta information about the host variables Ansible will not call it with \u0026ndash;host. The curl command is of course the place where your script would get data by whatever means, format it as proper JSON and return it (-sS makes curl silent, except for error messages).\nBut before we integrate the custom inventory script into our controller cluster, it’s a good idea to test it on the command line first:\n Bring up your VSCode browser tab. In either the visual editor or in the terminal using your favorite commandline editor, Create the file dyninv.sh with the content shown above. Make the script executable:  [lab-user@bastion ~]$ chmod +x dyninv.sh  Execute it:  [lab-user@bastion ~]$ ./dyninv.sh --list { \u0026#34;dyngroup\u0026#34;:{ \u0026#34;hosts\u0026#34;:[ \u0026#34;cloud1.cloud.example.com\u0026#34;, \u0026#34;cloud2.cloud.example.com\u0026#34; ], \u0026#34;vars\u0026#34;:{ \u0026#34;var1\u0026#34;: true } }, \u0026#34;_meta\u0026#34;:{ \u0026#34;hostvars\u0026#34;:{ \u0026#34;cloud1.cloud.example.com\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;web\u0026#34; }, \u0026#34;cloud2.cloud.example.com\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;database\u0026#34; } } } } The script should output the JSON-formatted output shown above.\nAs simple as it gets, right? More information can be found on how to develop dynamic inventories.\nSo now you have a source of (slightly static) dynamic inventory data (talk about oxymoron…) and a script to fetch and pass it to controller. Now you need to get this into controller.\nIntegrate into controller In Ansible Tower up to version 3.8, you could create inventory scripts directly in the web UI. Since automation controller 4.0 the only way to get inventory scripts into controller is by putting the script into a source control repository.\nFor this lab the inventory script was already created in the Git repo you have configured as a Project earlier, so you can use this as-is.\nYou can directly proceed to adding the dynamic inventory and pointing it to the inventory script.\n  In the web UI, open Resources→Inventories.\n  To create a new custom inventory, click the button and click on Add inventory.\n  Fill in the needed data:\n Name: Cloud Inventory    Click Save\n  Change to the Sources tab and once more click the blue button.\n  Fill in the needed data:\n  Name: Cloud Inventory Script\n  Source: Sourced from a project\n  Project: AWX Project\n  Inventory file: inventory/inventory-script\n  enable Update on launch\n    Click on Save\n  Start the initial sync by clicking on Sync\n  Navigate to Views -\u0026gt; Jobs to watch the initial sync, the Type is Inventory Sync.\nAfter the inventory sync has finished investigate the new hosts which were added by it to your inventory, by navigating to Resources -\u0026gt; Hosts. You should find two new hosts: cloud1.cloud.example.com and cloud2.cloud.example.com.\nWhat is the take-away? Using this simple example you have:\n  Created a script to query an inventory source\n  Integrated the script into controller\n  Populated an inventory using the custom script\n  Smart Inventories You will most likely have inventories from different sources in your controller installation. Maybe you have a local CMDB, your virtualization management and your public cloud provider to query for managed systems. Imagine you now want to run automation jobs across these inventories on hosts matching certain search criteria.\nThis is where Smart Inventories come in. A Smart Inventory is a collection of hosts defined by a stored search. Search criteria can be host attributes (like groups) or facts (such as installed software, services, hardware or whatever information Ansible facts are collected). A Smart Inventory can be viewed like a standard inventory and used for job runs.\nAutomation controller 4.0 introduces a new UI to build these search filters without the need of advanced regular expression kung-fu.\nA Simple Smart Inventory Let’s start with a simple string example. In your controller web UI, open the Resources -\u0026gt; Inventories view. Then click the button and choose to create a new Smart Inventory. In the next view:\n  Name: Simple Smart Inventory\n  Click the magnifying glass icon next to Smart host filter\n  A window Perform a search to define a host filter opens, here you define the search query\n  To start with you can just use simple search terms. Try cloud or example.com as search terms and see what you get after hitting ENTER.\nSearch terms are automatically saved so make sure to hit Clear all filters to clear the saved search when testing expressions.\n Or what about searching by inventory groups? Switch from Name to Group and enter dyngroup into the search field. After hitting ENTER you should only see cloud1 and cloud2. Try to narrow it down further by e.g. adding a second filter for Name cloud2.\nWhen your search returns the results you want, hit Select for the Perform a search to define a host filter window and Save for the Smart Inventory. Now your Smart Inventory is usable for executing job templates!\nThere are many additional attributes you can create a filter for - including Ansible facts returned from your managed nodes - but that\u0026rsquo;s for another lab\u0026hellip;\n Challenge Change the Simple Smart Inventory filter to include only enabled hosts in your smart inventory. Hosts can be temporarily disabled, for example due to some maintenance work. We want to exclude them from our inventory.\nTo test the filter, go to Resources-\u0026gt;Hosts and disable cloud1.cloud.example.com by switching the slider button to the right to Off. Then open Resources-\u0026gt;Inventories-\u0026gt;Simple Smart Inventory, go to the Hosts tab and check the hosts.\nClick here for Solution   This is an advanced lab, no solution here. But check the previous lab and change the filter to Enabled and Yes.   "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-getting-started/7-wrap/",
	"title": "Wrap up",
	"tags": [],
	"description": "",
	"content": "Final Challenge or Putting it all Together This is the final challenge where we try to put most of what you have learned together.\nLet’s set the stage Your team responsible for web application deployments like what they see in automation controller. To use it in their environment they put together these requirements:\n  As the webservers can be used for either development purposes or in production, there has to be a way to flag them accordingly as stage dev or stage prod.\n Currently node1.\u0026lt;GUID\u0026gt;.internal and node3.\u0026lt;GUID\u0026gt;.internal should be used as a development systems and node2.\u0026lt;GUID\u0026gt;.internal in production.    Of course the content of the world famous application \u0026ldquo;index.html\u0026rdquo; will be different between dev and prod stages.\n  There should be a title on the page stating the environment.\n  There should be a content field.\n    The content writer wweb should have access to a survey to change the content for dev and prod servers.\n  The Git Repository All code is already in place - this is a automation controller lab after all and not about configuring Apache. Check out the Ansible Workshop Examples git repository again at https://github.com/ansible-labs-crew/playbooks_summit2021. You will find the playbook webcontent.yml, which calls the role role_webcontent.\nCompared to the previous Apache installation role there is a major difference: there are now two versions of an index.html template, and a task deploying the template file which has a variable as part of the template file name.\nHere are the files for you to review (path is relative to the Github repository):\n rhel/apache/roles/role_webcontent/templates/dev_index.html.j2  \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a development webserver, have fun!\u0026lt;/h1\u0026gt; {{ dev_content }} \u0026lt;/body\u0026gt;  rhel/apache/roles/role_webcontent/templates/prod_index.html.j2  \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a production webserver, take care!\u0026lt;/h1\u0026gt; {{ prod_content }} \u0026lt;/body\u0026gt;  rhel/apache/roles/role_webcontent/tasks/main.yml  Only the part deploying the template is shown\n [...] - name: Deploy index.html from template template: src: \u0026quot;{{ stage }}_index.html.j2\u0026quot; dest: /var/www/html/index.html notify: apache-restart Prepare Inventory There is of course more then one way to accomplish this, but here is what you should do:\n  Make sure all three hosts are in the inventory Webserver.\n  Define a variable stage with the value dev for the Webserver inventory:\n Add stage: dev to the Details of inventory Webserver by putting it into the Variables field beneath the three start-yaml dashes.    Make sure to add the variable to the inventory and not to a node!\n  In the same way add a variable stage: prod but this time only for node2.\u0026lt;GUID\u0026gt;.internal (go to the Hosts view of the inventory).  This way the host variable overrides the variable set at the Inventory level because it\u0026rsquo;s more specific and takes precedence.\n Create the Template   Create a new Template named Create Web Content that\n  Targets the Webserver inventory.\n  Uses the Playbook rhel/apache/webcontent.yml from the Ansible Workshop Examples Project.\n  Defines two variables: dev_content: default dev content and prod_content: default prod content in Variables.\n  Uses Workshop Credentials and runs with privilege escalation.\n    Save and run the template.\n  Check the results Execute curl to get the web content from each node in your VS Code terminal:\n[lab-user@bastion ~]$ curl -s http://node1.\u0026lt;GUID\u0026gt;.internal \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a development webserver, have fun!\u0026lt;/h1\u0026gt; default dev content \u0026lt;/body\u0026gt; [lab-user@bastion 0 /etc/ansible]$ curl -s http://node2.\u0026lt;GUID\u0026gt;.internal \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a production webserver, take care!\u0026lt;/h1\u0026gt; default prod content \u0026lt;/body\u0026gt; [lab-user@bastion 0 /etc/ansible]$ curl -s http://node3.\u0026lt;GUID\u0026gt;.internal \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a development webserver, have fun!\u0026lt;/h1\u0026gt; default dev content \u0026lt;/body\u0026gt; Add Survey   Add a survey to the Template to allow changing the variables dev_content and prod_content.\n  Add permissions to the Team Web Content so the Template Create Web Content can be executed by wweb.\n  Run the survey as user wweb.\n  Check the results again from your VS Code terminal using curl as above.\nSolution Solution NOT below   You have to figure this one out by yourself! ;-)\n  You have done all the required configuration steps in the lab already. If unsure, just refer back to the respective chapters.\nThe End Congratulations, you finished your labs! We hope you enjoyed your first encounter with automation controller as much as we enjoyed creating the labs.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-getting-started/7-bonus/",
	"title": "Bonus Labs",
	"tags": [],
	"description": "",
	"content": "You have finished the lab already. But it doesn’t have to end here. We prepared some slightly more advanced bonus labs for you to follow through if you like. So if you are done with the labs and still have some time, here are some more labs for you:\nBonus Lab: Ad Hoc Commands Create a new user testuser on node1 and node3 with a comment using an ad hoc command, make sure that it is not created on node2!\n  Find the parameters for the appropriate module using ansible-doc user (leave with q)\n  Use an Ansible ad hoc command to create the user with the comment Test D User\n  Use the command module with the proper invocation to find the userid\n  Delete the user and check it has been deleted\n  Remember privilege escalation…​\n Click here for Solution   Your commands could look like these:\n[ec2-user@autoctl1 ansible-files]$ ansible-doc -l | grep -i user [ec2-user@autoctl1 ansible-files]$ ansible-doc user [ec2-user@autoctl1 ansible-files]$ ansible node1,node3 -m user -a \u0026#34;name=testuser comment=\u0026#39;Test D User\u0026#39;\u0026#34; -b [ec2-user@autoctl1 ansible-files]$ ansible node1,node3 -m command -a \u0026#34; id testuser\u0026#34; -b [ec2-user@autoctl1 ansible-files]$ ansible node2 -m command -a \u0026#34; id testuser\u0026#34; -b [ec2-user@autoctl1 ansible-files]$ ansible node1,node3 -m user -a \u0026#34;name=testuser state=absent remove=yes\u0026#34; -b [ec2-user@autoctl1 ansible-files]$ ansible web -m command -a \u0026#34; id testuser\u0026#34; -b \n  Bonus Lab: Templates and Variables You have learned the basics about Ansible templates, variables and handlers. Let’s combine all of these.\nInstead of editing and copying httpd.conf why don’t you just define a variable for the listen port and use it in a template? Here is your job:\n  Define a variable listen_port for the web group with the value 8080 and another for node2 with the value 80 using the proper files.\n  Copy the httpd.conf file into the template httpd.conf.j2 that uses the listen_port variable instead of the hard-coded port number.\n  Write a Playbook that deploys the template and restarts Apache on changes using a handler.\n  Run the Playbook and test the result using curl.\n  Remember the group_vars and host_vars directories? If not, refer to the chapter Using Variables.\n Define the variables: Add this line to group_vars/web:\nlisten_port: 8080 Add this line to host_vars/node2:\nlisten_port: 80 Prepare the template:   Copy httpd.conf to httpd.conf.j2\n  Edit the Listen directive in httpd.conf.j2 to make it look like this:\n  [...] Listen {{ listen_port }} [...] Create the Playbook Create a playbook called apache_config_tpl.yml:\n--- - name: Apache httpd.conf hosts: web become: yes tasks: - name: Create Apache configuration file from template ansible.builtin.template: src: httpd.conf.j2 dest: /etc/httpd/conf/httpd.conf notify: - restart apache handlers: - name: restart apache ansible.builtin.service: name: httpd state: restarted Run and test First run the playbook itself, then run curl against node1 with port 8080 and node2 with port 80.\n[ec2-user@autoctl1 ansible-files]$ ansible-playbook apache_config_tpl.yml [...] [ec2-user@autoctl1 ansible-files]$ curl http://18.195.235.231:8080 \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a development webserver, have fun!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; [ec2-user@autoctl1 ansible-files]$ curl http://35.156.28.209:80 \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is a production webserver, take care!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; The End Congratulations, you finished your labs! We hope you enjoyed your first encounter with Ansible as much as we enjoyed creating the labs.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/8-structured-content/",
	"title": "Well Structured Content Repositories",
	"tags": [],
	"description": "",
	"content": "OPTIONAL EXERCISE It’s a common part of the learning curve for Ansible and automation controller: At some point you will have written so many playbooks that a need for structure comes up. Where to put the Playbooks, what about the Templates, Files and so on.\nThe main recommendations are:\n  Put your content in a version control system like Git or SVN. This comes naturally since Ansible code is usually in text form anyway, and thus can be managed easily.\n  Group your code by logical units, called \u0026ldquo;roles\u0026rdquo; in Ansible.\n Example: have all code, config templates and files for the apache web server in one role, and all code, configuration and sql statements for the database in another role. That way the code becomes much better to read and handle, and roles can be made re-usable and shared between projects, teams or with the global community.    Of course, what structure works best in the end depends on the individual requirements, but we will highlight some common ground rules which apply to almost all use cases.\nThe first recommendation is to separate specific code from reusable/generic code from data:\n  specific code: Playbooks and their direct dependencies which are not shared outside the realm of the project or team.\n  generic code: All content that will be used across multiple projects.\n  data: This is mostly the inventory or the inventory scripts and the corresponding variables for hosts and groups. In many use cases it is advisable to have a dedicated inventory for each life-cycle environment.\n  Data content files can be in the same Git repository, each in its own directory (e.g. dev, test, qa, prod). Alternatively, for example in larger environments or with dedicated teams per environment there can be one Git repository for each environment. We recommend to put special focus on splitting out host and group data.\n Be careful to not have separate code repositories for each environment. It would go against the purpose of testing the same code as you push it through your life-cycle, only varying the data / inventory. If you have difficulties to keep the same code throughout all your environments we recommend to re-think the structure of our code and what you put into your inventory.\n Example repository So, let’s get started with an example. The content and repo-structure in this lab is mostly aligned to the Ansible best practices and is explained in more detail there (we\u0026rsquo;ve had to simplify a bit for the lab).\nSince we want to store all content in a repository, we have to create a simplistic Git server on our control host. In a more typical environment, you would work with GitLab, Gitea, or any other commercial Git server.\n[ec2-user@autoctl1 ~]$ wget https://raw.githubusercontent.com/ansible-labs-summit-crew/structured-content/master/simple_git.yml [ec2-user@autoctl1 ~]$ ansible-playbook simple_git.yml Next we will clone the repository on the control host. To enable you to work with git on the command line the SSH key for user ec2-user was already added to the Git user git. Next, clone the repository on the control machine:\n[ec2-user@autoctl1 ~]$ git clone git@bastion.\u0026lt;GUID\u0026gt;.internal:projects/structured-content.git # Message \u0026quot;warning: You appear to have cloned an empty repository.\u0026quot; is OK and can be ignored [ec2-user@autoctl1 ~]$ git config --global push.default simple [ec2-user@autoctl1 ~]$ git config --global user.name \u0026quot;Your Name\u0026quot; [ec2-user@autoctl1 ~]$ git config --global user.email you@example.com [ec2-user@autoctl1 ~]$ cd structured-content/  The repository is currently empty. The three config commands are just there to avoid useless warnings from Git.\n You are now going to add some default directories and files:\n[ec2-user@autoctl1 structured-content]$ touch {staging,production}  This command creates two inventory files: in this case we have different stages with different hosts which we keep them in separate inventory files. Note that those files are right now still empty and need to be filled with content to work properly.\nIn the current setup we have two instances. Let’s assume that node1.\u0026lt;GUID\u0026gt;.internal is part of the staging environment, and node2.\u0026lt;GUID\u0026gt;.internal is part of the production environment. To reflect that in the inventory files, edit the two empty inventory files to look like this:\n[ec2-user@autoctl1 structured-content]$ cat staging [staging] node1.\u0026lt;GUID\u0026gt;.internal [ec2-user@autoctl1 structured-content]$ cat production [production] node2.\u0026lt;GUID\u0026gt;.internal  Next we add some directories:\n  directories for host and group variables\n  A roles directory where the main part of our automation logic will be in.\n  For demonstration purpose we also will add a library directory: it can contain Ansible code related to a project like custom modules, plugins, etc.\n[ec2-user@autoctl1 structured-content]$ mkdir -p {group_vars,host_vars,library,roles}\n  Now to the two roles we’ll use in this example. First we’ll create a structure where we’ll add content later. This can easily be achieved with the command ansible-galaxy: it creates role skeletons with all appropriate files, directories and so on already in place.\n[ec2-user@autoctl1 structured-content]$ ansible-galaxy init --offline --init-path=roles security [ec2-user@autoctl1 structured-content]$ ansible-galaxy init --offline --init-path=roles apache  Even if a good role is generally self-explanatory, it still makes sense to have proper documentation. The right location to document roles is the file meta/main.yml.\n The roles are empty, so we need to add a few tasks to each. In the last chapters we set up an Apache webserver and used some security tasks. Let’s add that code to our roles by editing the two task files:\n[ec2-user@autoctl1 structured-content]$ cat roles/apache/tasks/main.yml --- # tasks file for apache - name: latest Apache version installed yum: name: httpd state: latest - name: latest firewalld version installed yum: name: firewalld state: latest - name: firewalld enabled and running service: name: firewalld enabled: true state: started - name: firewalld permits http service firewalld: service: http permanent: true state: enabled immediate: yes - name: Apache enabled and running service: name: httpd enabled: true state: started [ec2-user@autoctl1 structured-content]$ cat roles/security/tasks/main.yml --- # tasks file for security - name: \u0026quot;HIGH | RHEL-07-010290 | PATCH | The Red Hat Enterprise Linux operating system must not have accounts configured with blank or null passwords.\u0026quot; replace: dest: \u0026quot;{{ item }}\u0026quot; follow: true regexp: 'nullok ?' with_items: - /etc/pam.d/system-auth - /etc/pam.d/password-auth - name: \u0026quot;MEDIUM | RHEL-07-010210 | PATCH | The Red Hat Enterprise Linux operating system must be configured to use the shadow file to store only encrypted representations of passwords.\u0026quot; lineinfile: dest: /etc/login.defs regexp: ^#?ENCRYPT_METHOD line: \u0026quot;ENCRYPT_METHOD SHA512\u0026quot; - name: \u0026quot;SCORED | 1.1.1.2 | PATCH | Remove freevxfs module\u0026quot; modprobe: name: freevxfs state: absent  We also need to create a playbook to call the roles from. This is often call site.yml, since it keeps the main code for the setup of our environment. Create the file:\n[ec2-user@autoctl1 structured-content]$ cat site.yml --- - name: Execute apache and security roles hosts: all roles: - { role: apache } - { role: security }  So we have prepared a basic structure for quite some content - call tree to look at it.\nClick here for Solution   [ec2-user@autoctl1 structured-content]$ tree . ├── group_vars ├── host_vars ├── library ├── production ├── roles │ ├── apache │ │ ├── defaults │ │ │ └── main.yml │ │ ├── files │ │ ├── handlers │ │ │ └── main.yml │ │ ├── meta │ │ │ └── main.yml │ │ ├── README.md │ │ ├── tasks │ │ │ └── main.yml │ │ ├── templates │ │ ├── tests │ │ │ ├── inventory │ │ │ └── test.yml │ │ └── vars │ │ └── main.yml │ └── security │ ├── defaults │ │ └── main.yml │ ├── files │ ├── handlers │ │ └── main.yml │ ├── meta │ │ └── main.yml │ ├── README.md │ ├── tasks │ │ └── main.yml │ ├── templates │ ├── tests │ │ ├── inventory │ │ └── test.yml │ └── vars │ └── main.yml ├── site.yml └── staging  In real life, you should remove the unnecessary roles sub-directories to keep the structure easier to understand and maintain.\n   Since we so far created the code only locally on the control host, we need to add it to the repository and push it:\n[ec2-user@autoctl1 structured-content]$ git add production roles site.yml staging [ec2-user@autoctl1 structured-content]$ git commit -m \u0026#34;Adding inventories and apache security roles\u0026#34; [ec2-user@autoctl1 structured-content]$ git push Launch it! From the Command Line The code can now be launched. We start at the command line. Call the playbook site.yml with the appropriate inventory and privilege escalation:\n[root@bastion structured-content]$ ansible-playbook -i staging site.yml -b  Watch how the changes are done to the target machines. Afterwards, we could similarly execute the playbook against the production stage, but we want to keep something for controller to do, so we just check it:\n[root@bastion structured-content]$ ansible-playbook -i production site.yml -b --list-hosts --list-tasks  Call e.g. curl node1.\u0026lt;GUID\u0026gt;.internal to get the default page.\nFrom controller To configure and use this repository as a Source Control Management (SCM) system in controller you have to create credentials again, this time to access the Git repository over SSH. This credential is user/key based, and we need the following awx command (assuming the TOWER_ environment variables are still defined):\n[root@autoctl1 ~]# awx -f human credential create --name \u0026#34;Git Credentials\u0026#34; \\ --organization \u0026#34;Default\u0026#34; \\  --credential_type \u0026#34;Source Control\u0026#34; \\  --inputs \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;git\u0026#34;, \u0026#34;ssh_key_data\u0026#34;: \u0026#34;@~/.ssh/aws-private.pem\u0026#34;}\u0026#39; The new repository needs to be added as project. Feel free to use the web UI or use awx like shown below.\n[root@autoctl1 ~]# awx -f human project create --name \u0026#34;Structured Content Repository\u0026#34; \\ --organization Default \\  --scm_type git \\  --scm_url git@bastion.\u0026lt;GUID\u0026gt;.internal:projects/structured-content.git \\  --scm_clean 1 \\  --scm_update_on_launch 1 \\  --credential \u0026#34;Git Credentials\u0026#34; Now you’ve created the Project in controller. Earlier on the command line you’ve setup a staged environment by creating and using two different inventory files. But how can we get the same setup in controller? We use another way to define Inventories! It is possible to use inventory files provided in a SCM repository as an inventory source. This way we can use the inventory files we keep in Git.\nIn your controller web UI, open the RESOURCES→Inventories view. Then click the button and choose to create a new Inventory. In the next view:\n  NAME: Structured Content Inventory\n  Click SAVE\n  Click the button SOURCES which is now active at the top\n  Click the button (the top right one)\n  NAME: Production\n  SOURCE: Pick Sourced from a Project\n  PROJECT: Structured Content Repository\n  In the INVENTORY FILE drop down menu, pick production\n  Click the green SAVE button\n  And now for the staging inventory:\n  Down below in the view, click the button again\n  In the next view, add as NAME: Staging\n  SOURCE: Pick Sourced from a Project\n  PROJECT: Structured Content Repository\n  In the INVENTORY FILE drop down menu, pick staging\n  Click the green SAVE button\n  In the screen below, click the sync button for both sources, or SYNC ALL once so that the cloud icon on the left site next to the name of each inventory turns green.\n  To make sure that the project based inventory worked, click on the HOSTS button of the Inventory and make sure the two hosts are listed and tagged with the respective stages as RELATED GROUPS.\nNow create a template to execute the site.yml against both stages at the same time and associate the credentials.\nPlease note that in a real world use case you might want to have different templates to address the different stages separably.\n[ec2-user@autoctl1 ~]# awx -f human job_template create --name \u0026#34;Structured Content Execution\u0026#34; \\ --job_type run --inventory \u0026#34;Structured Content Inventory\u0026#34; \\  --project \u0026#34;Structured Content Repository\u0026#34; \\  --playbook \u0026#34;site.yml\u0026#34; \\  --become_enabled 1 [ec2-user@autoctl1 ~]# awx -f human job_template associate --name \u0026#34;Structured Content Execution\u0026#34; \\ --credential \u0026#34;Example Credentials\u0026#34;  Now in the controller web UI go to RESOURCES→Templates, launch the job template Structured Content Execution and watch the results.\nAdding External Roles So far we have only worked with content inside a single repository. While this drastically reduces complexity already, the largest benefit is in sharing roles among multiple teams or departments and keeping them in a central place. In this section we will show how to reference shared roles in your code and execute them together on your behalf.\nIn enterprise environments it is common to share roles via internal git repositories, often one git repository per role. If a role might be interesting and re-used by the world wide Ansible community, they can be shared on our central platform Ansible Galaxy. The advantage of Ansible Galaxy is that it features basic automatic testing and community ratings to give the interested users an idea of the quality and reusability of a role.\nTo use external roles in a project, they need to be referenced in a file called roles/requirements.yml, for example like this:\n# Import directly from Galaxy - src: geerlingguy.nginx # Import from a local Git repository - src: http://control.example.com/gitea/git/external-role.git version: master name: external-role_locally The requirements.yml needs to be read - either on the command line by invoking ansible-galaxy, or automatically by automation controller during project check outs. In both cases the file is read, and the roles are checked out and stored locally, and the roles can be called in playbooks. The advantage of controller here is that it takes care of all that - including authorization to the Git repo, finding a proper place to store the role, updating it when needed and so on.\nIn this example, we will include a role which ships a simple index.html file as template and reloads the apache web server. The role is already shared in GitHub at https://github.com/ansible-labs-summit-crew/shared-apache-role.\nTo include it with the existing structured content, first we have to create a file called roles/requirements.yml and reference the role there:\nMake sure you work as user student\u0026lt;GUID\u0026gt;\n Let\u0026rsquo;s create a roles/requirements.yml file:\n[ec2-user@autoctl1 structured-content]$ cat roles/requirements.yml - src: https://github.com/ansible-labs-summit-crew/shared-apache-role.git scm: git version: master  In a production environment you may want to change the version to a fixed version or tag, to make sure that only tested and verified code is checked out and used. But this strongly depends on how you develop your code and which branching model you use.\n Next, we reference the role itself in our playbook. Change the site.yml Playbook to look like this:\n[ec2-user@autoctl1 structured-content]$ cat site.yml --- - name: Execute apache and security roles hosts: all roles: - { role: apache} - { role: security } - { role: shared-apache-role } Because controller uses your Git repo, you’ve to add, commit and push the changes:\n[ec2-user@autoctl1 structured-content]$ git add site.yml roles/ [ec2-user@autoctl1 structured-content]$ git commit -m \u0026#34;Add roles/requirements.yml referencing shared role\u0026#34; [ec2-user@autoctl1 structured-content]$ git push Launch in controller Just in case, make sure to update the Project in controller: in the menu at RESOURCES, pick Projects, and click on the sync button next to Structured Content Repository.\nAfterwards, go to RESOURCES→Templates and launch the Structured Content Execution job template. As you will see in the job output, the external role is called just the way the other roles are called:\nTASK [shared-apache-role : deploy content] ************************************* changed: [node2.\u0026lt;GUID\u0026gt;.internal] changed: [node1.\u0026lt;GUID\u0026gt;.internal]  Validate again with curl the result and you are done!\nThis was quite something to follow through, so let’s review:\n  You successfully integrated a shared role provided from a central source into your automation code.\n  This way, you can limit your automation code to things really relevant and individual to the task and your environment, while everything generic is consumed from a shared resource.\n  "
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-controller-advanced/9-rest-api/",
	"title": "Discovering the automation controller API",
	"tags": [],
	"description": "",
	"content": "OPTIONAL EXERCISE You have used the controller API a couple of times in this lab already. In this chapter we’ll describe two ways to discover the controller API if you need to dive in deeper. While the principles of the Tower API are documented and there is an API reference guide, it’s often more efficient to just browse and discover the API.\nBrowsing and Using the controller API interactively The controller API is browsable, which means you can just click your way through it:\n  Go to the controller UI in your browser and make sure you’re logged in as admin.\n  Replace the end of the URL with /api e.g. https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com/api\n  There is currently only one API valid, so while in /api/v2:\n  you see a list of clickable object types\n  on the right upper side, there is a button OPTIONS which tells you what you can do with the current object in terms of API.\n  next to it there is a GET button which allows you to choose between getting the (raw or not) JSON output or the API format, which you’re currently admiring by default.\n    Click on the /api/v2/users/ link and discover some more features:\n  There is a list of all objects of the given type\n  Each individual object can be reached using the url field (\u0026ldquo;url\u0026rdquo;: \u0026ldquo;/api/v2/users/1/\u0026quot;,)\n  Most objects have a related field, which allows you to jump from object to object\n  At the bottom of the page, there is a new field which allows you to post a new object, so let’s do this and create a new user name John Smith (user name doesn’t matter)\n    Click here for Solution   The JSON should roughly look like this:\n{ \u0026#34;username\u0026#34;: \u0026#34;jsmith\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;jsmith@example.com\u0026#34;, \u0026#34;is_superuser\u0026#34;: false, \u0026#34;is_system_auditor\u0026#34;: false, \u0026#34;password\u0026#34;: \u0026#34;redhat\u0026#34; } and the result should be a 201 telling you about your success. You can login with the password and see that you see… nothing, because you have no rights.\n  Now log in again as admin and go back to the list of users: https://autoctl1.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com/api/v2/users/\n  Click on the url field of your new friend John Smith and notice a few more things:\n  There is a red DELETE button at the top right level. Guess for what?\n  At the bottom of the page, the dialog shows PUT and PATCH buttons.\n    So why not patch the user to be named \u0026ldquo;Johnny\u0026rdquo; instead of \u0026ldquo;John\u0026rdquo;?\nClick here for Solution   Add this to the CONTENT field:\n{ \u0026#34;first_name\u0026#34;: \u0026#34;Johnny\u0026#34; } And press the PATCH button.\n  Now try to PUT the last_name \u0026ldquo;Smithy\u0026rdquo; using the same approach. What happens?\nClick here for Solution   Enter this into the CONTENT field and press PUT:\n{ \u0026#34;last_name\u0026#34;: \u0026#34;Smithy\u0026#34; } This will fail. In the case of PUT you need to enter all mandatory fields, even if you don’t want to modify them:\n{ \u0026#34;username\u0026#34;: \u0026#34;jsmith\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Smithy\u0026#34; } \n  When you’re done press the red DELETE button and remove Johnny Smithy.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/vscode-intro/",
	"title": "Visual Studio Code - Introduction",
	"tags": [],
	"description": "",
	"content": "To make it easier for you to perform the tasks described in this workshop, we provide an Visual Studio Code Server for you. This web based IDE allows you to write and edit your Ansible Playbooks and execute command with the built-in Terminal. This part of the Workshop documentation will give you a very quick introduction.\nTested platforms There are probably thousands of combinations of Operating Systems and Web Browsers, and we couldn\u0026rsquo;t test them all. We noticed it works best with Google Chrome (or Chromium) on Linux, Windows or Mac. We ran into issues with Copy \u0026amp; Paste with Firefox and Edge on Windows 10. There might be other issues with other combinations, which is why we recommend to use Chrome/Chromium if you run into problems.\nHow to login Each students has it\u0026rsquo;s dedicated lab environment. This also means you have your dedicated instance of Visual Studio Code Server. The URL to open the Web Interface is:\nhttps://bastion.\u0026lt;GUID\u0026gt;.\u0026lt;SANDBOXID\u0026gt;.opentlc.com  Replace \u0026lt;N\u0026gt; with your student number and \u0026lt;LABID\u0026gt;. You will find your LABID in the invite to this event. If you don\u0026rsquo;t know your LABID, ask your instructor.\nAfter opening the URL, you will see a dialog asking for your password. The password was provided to you in the Lab instructions or the invite to this event.\nIf the password was entered correctly, you will see a web page like this:\nUI Basics On the very left you will see a column of Icons to switch the UI into different modes. During this workshop you will only need the Explorer mode indicated by the two paper icon.\nOn the upper right you should see a large window with a VS Code Welcome message. When you will start working with files, this will show the file you are working on.\nFinally in the lower right you can enable a Terminal session. If you can\u0026rsquo;t see the terminal view, click on Terminal and New Terminal in the menu navigation on the top.\nAs usual, you have a menu bar on the top and you can resize the three parts of the UI by moving the dividers.\nHow to use the Terminal You can execute commands on the Linux command line by using the terminal mode. It\u0026rsquo;s usually shown on the lower right part of the UI. If you can\u0026rsquo;t see the terminal view, click on Terminal and New Terminal in the menu navigation on the top.\nThe terminal is running on your \u0026ldquo;control node\u0026rdquo; or \u0026ldquo;ansible\u0026rdquo; node - this is how we call this instance in the Workshop Guide.\nIt\u0026rsquo;s a fully functional terminal with all the features you would expect from Linux. So, yes, you can break and destroy your instance if you try hard enough.\nWe noticed issues with Copy \u0026amp; Paste. On Linux the best way to paste text is to click the middle mouse button. On Windows 10 in our testing it only worked with Chrome and not with Firefox nor Edge.\n How to create and edit files There are several tasks in the workshop where you will be asked to create and edit files. You can create new files easily by clicking on File, New File. You can also save your files. Pay attention to the lab instructions since you are working in different directories during your lab exercises.\nBy clicking on the Explorer Mode icon (the two paper sheets in the button column on the far left) you can enable and disable a folder tree, which might help you to navigate between different files and folders.\nGotchas Runs best on Chrome Visual Studio Code Server should work well with all modern browsers like Firefox or Chrome. Due to limited resources we did not test all possible combinations. If you run into issues we recommend to use Google Chrome or Chromium.\nIf you don\u0026rsquo;t have Chrome installed, it is available here.\nCopy \u0026amp; Paste and general performance Please also keep in mind that the lab is running in the cloud. Sometimes performance is degraded due to high latency. This became particularly challenging when you try to copy \u0026amp; paste something into the editor window. After pressing Ctrl-V to paste, give a few seconds to respond.\nIn Chrome, you might have to allow paste from clipboard, when you use it the first time.\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Available Labs Ansible Getting Started\nAnsible automation controller Getting Started\nAnsible automation controller Advanced\nAnsible Collections\nVisual Studio Code - Introduction\n"
},
{
	"uri": "https://ansible-labs-crew.github.io/ansible-collections/6-automation-hub-and-galaxy/screenshots/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://ansible-labs-crew.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://ansible-labs-crew.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]